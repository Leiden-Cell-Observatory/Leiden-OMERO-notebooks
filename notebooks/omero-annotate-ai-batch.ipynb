{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMERO Micro-SAM Annotation Workflow\n",
    "\n",
    "This notebook demonstrates the new streamlined workflow using the `omero-annotate-ai` package focused on micro-SAM.\n",
    "\n",
    "### Instructions:\n",
    "- **Connection**: Use the secure connection widget below - no more manual password entry!\n",
    "- **Processing**: This notebook supports processing images from various OMERO object types: images, datasets, projects, plates, and screens.\n",
    "- **Configuration**: Specify the container type and ID using the interactive widget below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import the Package\n",
    "\n",
    "**Note**: If you haven't installed the package yet, run:\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "For OMERO functionality, also install:\n",
    "```bash\n",
    "pip install -e .[omero]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ omero-annotate-ai version: 0.1.0\n",
      "ğŸ”— OMERO functionality: âœ… Available\n",
      "ğŸ” Secure connection widget: âœ… Available\n",
      "ğŸ”‘ Keyring support: âœ… Available\n"
     ]
    }
   ],
   "source": [
    "# Import the new package\n",
    "import omero_annotate_ai\n",
    "from omero_annotate_ai import create_config_widget, create_pipeline, create_default_config, create_omero_connection_widget\n",
    "\n",
    "# OMERO-related imports\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "try:\n",
    "    import ezomero\n",
    "    OMERO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  ezomero not available. Install with: pip install -e .[omero]\")\n",
    "    OMERO_AVAILABLE = False\n",
    "\n",
    "# System and utility imports\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(f\"ğŸ“¦ omero-annotate-ai version: {omero_annotate_ai.__version__}\")\n",
    "print(f\"ğŸ”— OMERO functionality: {'âœ… Available' if OMERO_AVAILABLE else 'âŒ Not available'}\")\n",
    "print(f\"ğŸ” Secure connection widget: âœ… Available\")\n",
    "\n",
    "# Check keyring availability\n",
    "try:\n",
    "    import keyring\n",
    "    print(f\"ğŸ”‘ Keyring support: âœ… Available\")\n",
    "except ImportError:\n",
    "    print(f\"ğŸ”‘ Keyring support: âš ï¸ Not available (manual password entry only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Connection with OMERO\n",
    "\n",
    "The package now provides a secure, user-friendly widget for OMERO connections with the following features:\n",
    "\n",
    "### ğŸ” Security Features:\n",
    "- **Secure password storage** using OS-native keychain (Windows Credential Manager, macOS Keychain, Linux GNOME Keyring)\n",
    "- **Password expiration** options from 1 hour to permanent\n",
    "- **No plain-text passwords** in configuration files\n",
    "\n",
    "### ğŸ”— Integration Features:\n",
    "- **Auto-loading** from existing `.env` and `.ezomero` files\n",
    "- **Connection testing** before saving credentials\n",
    "- **Backward compatibility** with existing workflows\n",
    "\n",
    "### ğŸ¯ User Experience:\n",
    "- **Interactive widget** with clear visual feedback\n",
    "- **Show/hide password** toggle\n",
    "- **One-click connection** with status indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ OMERO Connection Setup\n",
      "Use the widget below to connect to your OMERO server securely:\n",
      "  â€¢ Auto-loads from .env and .ezomero files\n",
      "  â€¢ Secure password storage in OS keychain\n",
      "  â€¢ Password expiration options\n",
      "  â€¢ Connection testing before saving\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8340e756c6941ec92e666ce1800b7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ğŸ”Œ OMERO Server Connection</h3>', layout=Layout(margin='0 0 20px 0')), HTML(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Instructions:\n",
      "1. Fill in your OMERO server details (host, username, password)\n",
      "2. Choose 'Remember password for' option if desired\n",
      "3. Click 'Test Connection' to verify your credentials\n",
      "4. Click 'Save & Connect' to establish the connection\n",
      "5. Continue to the next cell once connected\n"
     ]
    }
   ],
   "source": [
    "# Create OMERO connection using the new secure widget\n",
    "from omero_annotate_ai import create_omero_connection_widget\n",
    "\n",
    "print(\"ğŸ”Œ OMERO Connection Setup\")\n",
    "print(\"Use the widget below to connect to your OMERO server securely:\")\n",
    "print(\"  â€¢ Auto-loads from .env and .ezomero files\")\n",
    "print(\"  â€¢ Secure password storage in OS keychain\")\n",
    "print(\"  â€¢ Password expiration options\")\n",
    "print(\"  â€¢ Connection testing before saving\")\n",
    "\n",
    "# Create and display the connection widget\n",
    "conn_widget = create_omero_connection_widget()\n",
    "conn_widget.display()\n",
    "\n",
    "print(\"\\nğŸ“ Instructions:\")\n",
    "print(\"1. Fill in your OMERO server details (host, username, password)\")\n",
    "print(\"2. Choose 'Remember password for' option if desired\")\n",
    "print(\"3. Click 'Test Connection' to verify your credentials\")\n",
    "print(\"4. Click 'Save & Connect' to establish the connection\")\n",
    "print(\"5. Continue to the next cell once connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "âŒ No OMERO connection established. Please use the widget above to connect.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m conn \u001b[38;5;241m=\u001b[39m conn_widget\u001b[38;5;241m.\u001b[39mget_connection()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ No OMERO connection established. Please use the widget above to connect.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… OMERO connection ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ‘¤ User: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mgetUser()\u001b[38;5;241m.\u001b[39mgetName()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mConnectionError\u001b[0m: âŒ No OMERO connection established. Please use the widget above to connect."
     ]
    }
   ],
   "source": [
    "# Get the OMERO connection from the widget\n",
    "conn = conn_widget.get_connection()\n",
    "\n",
    "if conn is None:\n",
    "    raise ConnectionError(\"âŒ No OMERO connection established. Please use the widget above to connect.\")\n",
    "\n",
    "print(\"âœ… OMERO connection ready!\")\n",
    "print(f\"ğŸ‘¤ User: {conn.getUser().getName()}\")\n",
    "print(f\"ğŸ¢ Group: {conn.getGroupFromContext().getName()}\")\n",
    "\n",
    "# Create temporary work directory\n",
    "import tempfile\n",
    "import os\n",
    "output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print(f\"ğŸ“ Created temporary work directory: {output_directory}\")\n",
    "\n",
    "# Show connection status\n",
    "print(f\"\\nğŸ”— Connection Status:\")\n",
    "print(f\"   Host: {conn._BlitzGateway__ic.getProperties().getProperty('omero.host')}\")\n",
    "print(f\"   Secure: {conn.isSecure()}\")\n",
    "print(f\"   Keep-alive: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Configuration\n",
    "\n",
    "Use the interactive widget to configure your micro-SAM annotation workflow. The widget provides:\n",
    "- **OMERO Connection**: Specify container type and ID\n",
    "- **Micro-SAM Model**: Choose model type (vit_b, vit_l, vit_h, vit_b_lm)\n",
    "- **Image Processing**: Configure timepoints, z-slices, channels, 3D mode\n",
    "- **Patches**: Optional patch extraction settings\n",
    "- **Training**: Data organization and validation\n",
    "- **Workflow**: Resume options and output settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb6dc05bc2d455ea96b8b494ec3a8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ğŸ”¬ OMERO micro-SAM Configuration</h3>', layout=Layout(margin='0 0 20px 0')), Accâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Configure your micro-SAM annotation workflow using the widget above:\n",
      "   1. Set your OMERO container details (Dataset ID, etc.)\n",
      "   2. Choose your micro-SAM model (vit_b, vit_l, vit_h, vit_b_lm)\n",
      "   3. Adjust processing parameters as needed\n",
      "   4. Click 'Update Configuration' to apply changes\n",
      "   5. Click 'Validate' to check your configuration\n",
      "   6. Click 'Show YAML' to see the configuration\n"
     ]
    }
   ],
   "source": [
    "# Create the configuration widget\n",
    "config_widget = create_config_widget()\n",
    "config_widget.display()\n",
    "\n",
    "print(\"ğŸ“ Configure your micro-SAM annotation workflow using the widget above:\")\n",
    "print(\"   1. Set your OMERO container details (Dataset ID, etc.)\")\n",
    "print(\"   2. Choose your micro-SAM model (vit_b, vit_l, vit_h, vit_b_lm)\")\n",
    "print(\"   3. Adjust processing parameters as needed\")\n",
    "print(\"   4. Click 'Update Configuration' to apply changes\")\n",
    "print(\"   5. Click 'Validate' to check your configuration\")\n",
    "print(\"   6. Click 'Show YAML' to see the configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: Set Training Set Name\n",
    "\n",
    "Choose a specific name for your training set. This helps with organization and allows resuming interrupted workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Training Set Name: training_data_20240618\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m config_widget\u001b[38;5;241m.\u001b[39mget_config()\n\u001b[0;32m     12\u001b[0m config\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mtrainingset_name \u001b[38;5;241m=\u001b[39m trainingset_name\n\u001b[1;32m---> 13\u001b[0m config\u001b[38;5;241m.\u001b[39mbatch_processing\u001b[38;5;241m.\u001b[39moutput_folder \u001b[38;5;241m=\u001b[39m \u001b[43moutput_directory\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ Output directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_directory' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set a name for the training set\n",
    "# Use a specific name if you want to resume from an existing table\n",
    "trainingset_name = \"training_data_20240618\"  # Fixed name for resuming\n",
    "# trainingset_name = \"training_data_\" + pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")  # Auto-generated name\n",
    "\n",
    "print(f\"ğŸ¯ Training Set Name: {trainingset_name}\")\n",
    "\n",
    "# Update the configuration with the training set name\n",
    "config = config_widget.get_config()\n",
    "config.training.trainingset_name = trainingset_name\n",
    "config.batch_processing.output_folder = output_directory\n",
    "\n",
    "print(f\"ğŸ“ Output directory: {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate Configuration and Show Preview\n",
    "\n",
    "Before running the pipeline, let's validate the configuration and preview what will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration from widget\n",
    "config = config_widget.get_config()\n",
    "config.training.trainingset_name = trainingset_name\n",
    "config.batch_processing.output_folder = output_directory\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config.validate()\n",
    "    print(\"âœ… Configuration is valid!\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ Configuration validation failed:\")\n",
    "    print(f\"   {e}\")\n",
    "    raise\n",
    "\n",
    "# Show configuration summary\n",
    "print(\"\\nğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   ğŸ”¬ Model: {config.microsam.model_type}\")\n",
    "print(f\"   ğŸ“¦ Container: {config.omero.container_type} (ID: {config.omero.container_id})\")\n",
    "print(f\"   ğŸ“º Channel: {config.omero.channel}\")\n",
    "print(f\"   ğŸ¯ Training Set: {config.training.trainingset_name}\")\n",
    "print(f\"   ğŸ“Š Batch Size: {config.batch_processing.batch_size} (0 = all in one batch)\")\n",
    "print(f\"   ğŸ§© Use Patches: {config.patches.use_patches}\")\n",
    "if config.patches.use_patches:\n",
    "    print(f\"   ğŸ“ Patch Size: {config.patches.patch_size}\")\n",
    "    print(f\"   ğŸ“ˆ Patches per Image: {config.patches.patches_per_image} (non-overlapping)\")\n",
    "print(f\"   ğŸ”„ Resume from Table: {config.workflow.resume_from_table}\")\n",
    "print(f\"   ğŸ“– Read-only Mode: {config.workflow.read_only_mode}\")\n",
    "print(f\"   ğŸ§  3D Processing: {config.microsam.three_d}\")\n",
    "\n",
    "# Create the pipeline and preview container contents\n",
    "pipeline = create_pipeline(config, conn)\n",
    "\n",
    "# Get container details\n",
    "container_type = config.omero.container_type\n",
    "container_id = config.omero.container_id\n",
    "\n",
    "# Validate container exists and get details\n",
    "if container_type == \"dataset\":\n",
    "    container = conn.getObject(\"Dataset\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Dataset with ID {container_id} not found\")\n",
    "    print(f\"\\nğŸ“ Dataset: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"project\":\n",
    "    container = conn.getObject(\"Project\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Project with ID {container_id} not found\")\n",
    "    print(f\"\\nğŸ“‚ Project: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"plate\":\n",
    "    container = conn.getObject(\"Plate\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Plate with ID {container_id} not found\")\n",
    "    print(f\"\\nğŸ§ª Plate: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"image\":\n",
    "    container = conn.getObject(\"Image\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Image with ID {container_id} not found\")\n",
    "    print(f\"\\nğŸ–¼ï¸ Image: {container.getName()} (ID: {container_id})\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unsupported container type: {container_type}\")\n",
    "\n",
    "# Get list of images that will be processed\n",
    "try:\n",
    "    images_list = pipeline.get_images_from_container()\n",
    "    print(f\"\\nğŸ“Š Found {len(images_list)} images to process\")\n",
    "    \n",
    "    # Show first few images\n",
    "    print(\"\\nğŸ–¼ï¸ Sample images:\")\n",
    "    for i, img in enumerate(images_list[:5]):\n",
    "        if hasattr(img, 'getName'):  # OMERO image object\n",
    "            print(f\"   {i+1}. {img.getName()} (ID: {img.getId()})\")\n",
    "        else:  # Image ID\n",
    "            img_obj = conn.getObject(\"Image\", img)\n",
    "            if img_obj:\n",
    "                print(f\"   {i+1}. {img_obj.getName()} (ID: {img})\")\n",
    "    \n",
    "    if len(images_list) > 5:\n",
    "        print(f\"   ... and {len(images_list) - 5} more images\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting images from container: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Project: Senescence (ID: 101)\n",
      "ğŸ“ Description: No description\n",
      "ğŸ“ Loading images from project 101\n",
      "ğŸ“Š Found 6 images\n",
      "\n",
      "ğŸ“Š Found 6 images to process\n",
      "\n",
      "ğŸ–¼ï¸ Sample images:\n",
      "   1. r01c06.tif (ID: 252)\n",
      "   2. r01c05.tif (ID: 255)\n",
      "   3. r01c02.tif (ID: 251)\n",
      "   4. r01c01.tif (ID: 256)\n",
      "   5. r01c04.tif (ID: 253)\n",
      "   ... and 1 more images\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline (but don't run it yet)\n",
    "pipeline = create_pipeline(config, conn)\n",
    "\n",
    "# Get container details\n",
    "container_type = config.omero.container_type\n",
    "container_id = config.omero.container_id\n",
    "\n",
    "# Validate container exists and get details\n",
    "if container_type == \"dataset\":\n",
    "    container = conn.getObject(\"Dataset\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Dataset with ID {container_id} not found\")\n",
    "    print(f\"ğŸ“ Dataset: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"project\":\n",
    "    container = conn.getObject(\"Project\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Project with ID {container_id} not found\")\n",
    "    print(f\"ğŸ“‚ Project: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"plate\":\n",
    "    container = conn.getObject(\"Plate\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Plate with ID {container_id} not found\")\n",
    "    print(f\"ğŸ§ª Plate: {container.getName()} (ID: {container_id})\")\n",
    "    print(f\"ğŸ“ Description: {container.getDescription() or 'No description'}\")\n",
    "    \n",
    "elif container_type == \"image\":\n",
    "    container = conn.getObject(\"Image\", container_id)\n",
    "    if container is None:\n",
    "        raise ValueError(f\"Image with ID {container_id} not found\")\n",
    "    print(f\"ğŸ–¼ï¸ Image: {container.getName()} (ID: {container_id})\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unsupported container type: {container_type}\")\n",
    "\n",
    "# Get list of images that will be processed\n",
    "try:\n",
    "    images_list = pipeline.get_images_from_container()\n",
    "    print(f\"\\nğŸ“Š Found {len(images_list)} images to process\")\n",
    "    \n",
    "    # Show first few images\n",
    "    print(\"\\nğŸ–¼ï¸ Sample images:\")\n",
    "    for i, img in enumerate(images_list[:5]):\n",
    "        if hasattr(img, 'getName'):  # OMERO image object\n",
    "            print(f\"   {i+1}. {img.getName()} (ID: {img.getId()})\")\n",
    "        else:  # Image ID\n",
    "            img_obj = conn.getObject(\"Image\", img)\n",
    "            if img_obj:\n",
    "                print(f\"   {i+1}. {img_obj.getName()} (ID: {img})\")\n",
    "    \n",
    "    if len(images_list) > 5:\n",
    "        print(f\"   ... and {len(images_list) - 5} more images\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting images from container: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting annotation pipeline...\n",
      "   This will process 6 images using micro-SAM\n",
      "   Model: vit_l\n",
      "   Napari will open for interactive annotation\n",
      "   Close napari windows when annotation is complete for each batch\n",
      "ğŸ“ Loading images from project 101\n",
      "ğŸ“Š Found 6 images\n",
      "ğŸš€ Starting micro-SAM annotation pipeline\n",
      "ğŸ“Š Processing 6 images with model: vit_l\n",
      "ğŸ“‹ Creating new tracking table: microsam_training_training_data_20240618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:101: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna(False).infer_objects(copy=False).astype(bool)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:106: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:106: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:106: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n",
      "/home/maarten/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/omero/omero_functions.py:106: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].fillna('None').infer_objects(copy=False).astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Created tracking table 'microsam_training_training_data_20240618' with 6 units\n",
      "   Container: project 101\n",
      "   Table ID: 670\n",
      "object group 0\n",
      "Stored configuration as annotation ID: 671\n",
      "ğŸ“‹ Getting unprocessed units from table 670\n",
      "ğŸ“‹ Found 6 unprocessed units\n",
      "ğŸ“‹ Found 6 processing units\n",
      "ğŸ”„ Processing batch 1/1\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n",
      "ğŸ“Š Loading 1 images using dask...\n",
      "ğŸ’¾ Materializing dask arrays to numpy (required for micro-SAM)...\n",
      "   Processing chunk 1/1\n",
      "âœ… Successfully loaded 1 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompute state for files:   0%|          | 0/6 [01:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Close napari windows when annotation is complete for each batch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Run the complete workflow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     table_id, processed_images = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Annotation pipeline completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:528\u001b[39m, in \u001b[36mAnnotationPipeline.run_full_workflow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images_list:\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.omero.container_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.omero.container_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:464\u001b[39m, in \u001b[36mAnnotationPipeline.run\u001b[39m\u001b[34m(self, images_list)\u001b[39m\n\u001b[32m    461\u001b[39m     batch_with_images.append((image_obj, seq_val, meta, row_idx))\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Run annotation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m annotation_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_microsam_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_with_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# Process results\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;28mself\u001b[39m._process_annotation_results(annotation_results, table_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/omero_annotate_ai/core/pipeline.py:284\u001b[39m, in \u001b[36mAnnotationPipeline._run_microsam_annotation\u001b[39m\u001b[34m(self, batch_data)\u001b[39m\n\u001b[32m    281\u001b[39m napari_settings.application.save_window_geometry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Run image series annotator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m segmentation_results = \u001b[43mimage_series_annotator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_volumetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmicrosam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthree_d\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    293\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: segmentation_results,\n\u001b[32m    294\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    295\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mzarr_path\u001b[39m\u001b[33m\"\u001b[39m: zarr_path,\n\u001b[32m    296\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33membedding_path\u001b[39m\u001b[33m\"\u001b[39m: embedding_path\n\u001b[32m    297\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/sam_annotator/image_series_annotator.py:180\u001b[39m, in \u001b[36mimage_series_annotator\u001b[39m\u001b[34m(images, output_folder, model_type, embedding_path, tile_shape, halo, viewer, return_viewer, precompute_amg_state, checkpoint_path, is_volumetric, device, prefer_decoder, skip_segmented)\u001b[39m\n\u001b[32m    177\u001b[39m os.makedirs(output_folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Precompute embeddings and amg state (if corresponding options set).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m predictor, decoder, embedding_paths = \u001b[43m_precompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_volumetric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m next_image_id = \u001b[32m0\u001b[39m\n\u001b[32m    188\u001b[39m have_inputs_as_arrays = \u001b[38;5;28misinstance\u001b[39m(images[next_image_id], np.ndarray)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/sam_annotator/image_series_annotator.py:46\u001b[39m, in \u001b[36m_precompute\u001b[39m\u001b[34m(images, model_type, embedding_path, tile_shape, halo, precompute_amg_state, checkpoint_path, device, ndim, prefer_decoder)\u001b[39m\n\u001b[32m     44\u001b[39m     embedding_paths = [\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43m_precompute_state_for_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(images[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m     51\u001b[39m         embedding_paths = [\n\u001b[32m     52\u001b[39m             os.path.join(embedding_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33membedding_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m05\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.zarr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images)\n\u001b[32m     53\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/precompute_state.py:213\u001b[39m, in \u001b[36m_precompute_state_for_files\u001b[39m\u001b[34m(predictor, input_files, output_path, key, ndim, tile_shape, halo, precompute_amg_state, decoder)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     out_path = os.path.join(output_path, os.path.basename(file_path))\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[43m_precompute_state_for_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecompute_amg_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m idx += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/precompute_state.py:166\u001b[39m, in \u001b[36m_precompute_state_for_file\u001b[39m\u001b[34m(predictor, input_path, output_path, key, ndim, tile_shape, halo, precompute_amg_state, decoder, verbose)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Precompute the image embeddings.\u001b[39;00m\n\u001b[32m    165\u001b[39m output_path = Path(output_path).with_suffix(\u001b[33m\"\u001b[39m\u001b[33m.zarr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m embeddings = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprecompute_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtile_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Precompute the state for automatic instance segmnetaiton (AMG or AIS).\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m precompute_amg_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/util.py:1064\u001b[39m, in \u001b[36mprecompute_image_embeddings\u001b[39m\u001b[34m(predictor, input_, save_path, lazy_loading, ndim, tile_shape, halo, verbose, batch_size, pbar_init, pbar_update)\u001b[39m\n\u001b[32m   1061\u001b[39m _, pbar_init, pbar_update, pbar_close = handle_pbar(verbose, pbar_init, pbar_update)\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tile_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     embeddings = \u001b[43m_compute_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tile_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1066\u001b[39m     embeddings = _compute_tiled_2d(input_, predictor, tile_shape, halo, f, pbar_init, pbar_update, batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/micro_sam/util.py:791\u001b[39m, in \u001b[36m_compute_2d\u001b[39m\u001b[34m(input_, predictor, f, save_path, pbar_init, pbar_update)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Otherwise we have to compute the embeddings.\u001b[39;00m\n\u001b[32m    790\u001b[39m predictor.reset_image()\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m features = predictor.get_image_embedding().cpu().numpy()\n\u001b[32m    793\u001b[39m original_size = predictor.original_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/predictor.py:60\u001b[39m, in \u001b[36mSamPredictor.set_image\u001b[39m\u001b[34m(self, image, image_format)\u001b[39m\n\u001b[32m     57\u001b[39m input_image_torch = torch.as_tensor(input_image, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m input_image_torch = input_image_torch.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/predictor.py:89\u001b[39m, in \u001b[36mSamPredictor.set_torch_image\u001b[39m\u001b[34m(self, transformed_image, original_image_size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = \u001b[38;5;28mtuple\u001b[39m(transformed_image.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m     88\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m.model.preprocess(transformed_image)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.is_image_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    109\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.neck(x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m     H, W = x.shape[\u001b[32m1\u001b[39m], x.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    172\u001b[39m     x, pad_hw = window_partition(x, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/micro-sam/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Starting annotation pipeline...\")\n",
    "print(f\"   This will process {len(images_list)} images using micro-SAM\")\n",
    "print(f\"   Model: {config.microsam.model_type}\")\n",
    "print(f\"   Napari will open for interactive annotation\")\n",
    "print(f\"   Close napari windows when annotation is complete for each batch\")\n",
    "\n",
    "try:\n",
    "    # Run the complete workflow\n",
    "    table_id, processed_images = pipeline.run_full_workflow()\n",
    "    \n",
    "    print(f\"\\nâœ… Annotation pipeline completed successfully!\")\n",
    "    print(f\"ğŸ“Š Processed {len(processed_images)} images\")\n",
    "    print(f\"ğŸ“‹ Tracking table ID: {table_id}\")\n",
    "    \n",
    "    if config.workflow.read_only_mode:\n",
    "        print(f\"ğŸ’¾ Annotations saved locally to: {config.workflow.local_output_dir}\")\n",
    "    else:\n",
    "        print(f\"â˜ï¸ Annotations uploaded to OMERO\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during annotation pipeline: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Annotation Pipeline\n",
    "\n",
    "Now we'll run the annotation workflow. This will:\n",
    "1. Create or resume from a tracking table\n",
    "2. Process images in batches (or all at once if batch_size=0)\n",
    "3. Launch napari for interactive annotation\n",
    "4. Upload results to OMERO (or save locally if in read-only mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ Starting annotation pipeline...\")\n",
    "print(f\"   This will process {len(images_list)} images using micro-SAM\")\n",
    "print(f\"   Model: {config.microsam.model_type} (default: vit_b_lm)\")\n",
    "if config.batch_processing.batch_size == 0:\n",
    "    print(f\"   Processing: All images in one batch\")\n",
    "else:\n",
    "    print(f\"   Processing: Batches of {config.batch_processing.batch_size} images\")\n",
    "print(f\"   Napari will open for interactive annotation\")\n",
    "print(f\"   Close napari windows when annotation is complete for each batch\")\n",
    "\n",
    "try:\n",
    "    # Run the complete workflow\n",
    "    table_id, processed_images = pipeline.run_full_workflow()\n",
    "    \n",
    "    print(f\"\\nâœ… Annotation pipeline completed successfully!\")\n",
    "    print(f\"ğŸ“Š Processed {len(processed_images)} images\")\n",
    "    print(f\"ğŸ“‹ Tracking table ID: {table_id}\")\n",
    "    \n",
    "    if config.workflow.read_only_mode:\n",
    "        print(f\"ğŸ’¾ Annotations saved locally to: {config.workflow.local_output_dir}\")\n",
    "    else:\n",
    "        print(f\"â˜ï¸ Annotations uploaded to OMERO\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during annotation pipeline: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Results\n",
    "\n",
    "Let's examine the tracking table and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration to file\n",
    "config_filename = f\"annotation_config_{trainingset_name}.yaml\"\n",
    "config.save_yaml(config_filename)\n",
    "print(f\"ğŸ’¾ Configuration saved to: {config_filename}\")\n",
    "\n",
    "# Display the YAML configuration\n",
    "print(\"\\nğŸ“„ YAML Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(config.to_yaml())\n",
    "\n",
    "# Show how to load it back\n",
    "print(\"\\nğŸ”„ To reuse this configuration:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from omero_annotate_ai import load_config\")\n",
    "print(f\"config = load_config('{config_filename}')\")\n",
    "print(f\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Export/Import\n",
    "\n",
    "Save your configuration for future use or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get micro-SAM specific parameters\n",
    "microsam_params = config.get_microsam_params()\n",
    "print(f\"ğŸ”§ Micro-SAM parameters:\")\n",
    "for key, value in microsam_params.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Show configuration for different models\n",
    "print(\"\\nğŸ¤– Available micro-SAM models:\")\n",
    "models = [\"vit_b\", \"vit_l\", \"vit_h\", \"vit_b_lm\"]\n",
    "for model in models:\n",
    "    print(f\"   â€¢ {model}\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ Current model: {config.microsam.model_type}\")\n",
    "print(f\"ğŸ§  3D processing: {config.microsam.three_d}\")\n",
    "print(f\"â° Timepoint mode: {config.microsam.timepoint_mode}\")\n",
    "print(f\"ğŸ”¢ Z-slice mode: {config.microsam.z_slice_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Tool-Specific Parameters\n",
    "\n",
    "The package supports different AI tools with tool-specific parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps & Advanced Usage\n",
    "\n",
    "### ğŸ”Œ OMERO Connection Widget\n",
    "This notebook now uses the new `create_omero_connection_widget()` for secure OMERO connections:\n",
    "\n",
    "```python\n",
    "from omero_annotate_ai import create_omero_connection_widget\n",
    "\n",
    "# Create connection widget\n",
    "conn_widget = create_omero_connection_widget()\n",
    "conn_widget.display()\n",
    "\n",
    "# Get connection after user interaction\n",
    "conn = conn_widget.get_connection()\n",
    "```\n",
    "\n",
    "### ğŸ” Keychain Integration\n",
    "- Passwords are securely stored in your OS keychain\n",
    "- Support for password expiration (1 hour to permanent)\n",
    "- Automatic loading from `.env` and `.ezomero` files\n",
    "- Fallback to manual entry if keychain unavailable\n",
    "\n",
    "### ğŸ› ï¸ Development Tips\n",
    "- Use `.env` files for development and testing\n",
    "- Save frequently used configurations as YAML files\n",
    "- Test connections before running large batches\n",
    "- Consider using `read_only_mode` for safe testing\n",
    "\n",
    "### ğŸ“š Documentation\n",
    "- Package documentation: See `CLAUDE.md` for development guidelines\n",
    "- Example scripts: Check `example_test/` folder for usage examples\n",
    "- API reference: Use `help(function_name)` for detailed documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "    print(f\"ğŸ—‘ï¸ Removed temporary directory: {output_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error removing temporary directory: {e}\")\n",
    "\n",
    "# Close OMERO connection\n",
    "if 'conn' in locals() and conn is not None:\n",
    "    conn.close()\n",
    "    print(\"ğŸ”Œ OMERO connection closed\")\n",
    "\n",
    "print(\"\\nâœ¨ Workflow completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clean Up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
