{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Stardist segmentation on 2D/3D/timelapse OMERO images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for Stardist segmentation. Some inspiration from the https://github.com/ome/omero-guide-cellprofiler/idr0002.ipynb\n",
    "\n",
    "## TO DO\n",
    "- Make a generic function for 2D segmentation for all slices independent of the shape of the image z,c,t\n",
    "- Include a ID to all files uploaded to OMERO to make it more tracable\n",
    "- Extend to handle multiple channels AND timepoints\n",
    "- Check if we can overwrite label images if nesseary or ROIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OMERO Python BlitzGateway\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Import Python System Packages\n",
    "import os\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "#stardist related\n",
    "from stardist.models import StarDist2D\n",
    "from csbdeep.utils import normalize\n",
    "from stardist.plot import render_label\n",
    "from tifffile import imsave\n",
    "\n",
    "#load stardist model\n",
    "model = StarDist2D.from_pretrained('2D_versatile_fluo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Temp Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print(new_output_directory)\n",
    "#create unique job id for reference based on date and time\n",
    "job_id = str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "conn = ezomero.connect(user=os.environ.get(\"USER_NAME\"),password=os.environ.get(\"PASSWORD\"),group=os.environ.get(\"GROUP\"),host=os.environ.get(\"HOST\"),port=os.environ.get(\"PORT\"),secure=True)\n",
    "connection_status = conn.connect()\n",
    "if connection_status:\n",
    "    print(\"Connected to OMERO Server\")\n",
    "else:\n",
    "    print(\"Connection to OMERO Server Failed\")\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"project\", \"plate\", \"dataset\" or \"image\"\n",
    "data_id = \t1112\n",
    "nucl_channel = 0\n",
    "\n",
    "def print_object_details(conn, obj, datatype):\n",
    "    \"\"\"Print detailed information about OMERO objects\"\"\"\n",
    "    print(f\"\\n{datatype.capitalize()} Details:\")\n",
    "    print(f\"- Name: {obj.getName()}\")\n",
    "    print(f\"- ID: {obj.getId()}\")\n",
    "    print(f\"- Owner: {obj.getOwner().getFullName()}\")\n",
    "    print(f\"- Group: {obj.getDetails().getGroup().getName()}\")\n",
    "    \n",
    "    if datatype == \"project\":\n",
    "        datasets = list(obj.listChildren())\n",
    "        dataset_count = len(datasets)\n",
    "        total_images = sum(len(list(ds.listChildren())) for ds in datasets)\n",
    "        print(f\"- Number of datasets: {dataset_count}\")\n",
    "        print(f\"- Total images: {total_images}\")\n",
    "        \n",
    "    elif datatype == \"plate\":\n",
    "        wells = list(obj.listChildren())\n",
    "        well_count = len(wells)\n",
    "        print(f\"- Number of wells: {well_count}\")\n",
    "        \n",
    "    elif datatype == \"dataset\":\n",
    "        images = list(obj.listChildren())\n",
    "        image_count = len(images)\n",
    "        # Get project info if dataset is in a project\n",
    "        projects = obj.getParent()\n",
    "        if projects:\n",
    "            print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Project: None (orphaned dataset)\")\n",
    "        print(f\"- Number of images: {image_count}\")\n",
    "        \n",
    "    elif datatype == \"image\":\n",
    "        size_x = obj.getSizeX()\n",
    "        size_y = obj.getSizeY()\n",
    "        size_z = obj.getSizeZ()\n",
    "        size_c = obj.getSizeC()\n",
    "        size_t = obj.getSizeT()\n",
    "        # Get dataset info if image is in a dataset\n",
    "        datasets = obj.getParent()\n",
    "        if datasets:\n",
    "            print(f\"- Dataset: {datasets.getName()} (ID: {datasets.getId()})\")\n",
    "            # Get project info if dataset is in a project\n",
    "            projects = datasets.getParent()\n",
    "            if projects:\n",
    "                print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Dataset: None (orphaned image)\")\n",
    "        print(f\"- Dimensions: {size_x}x{size_y}\")\n",
    "        print(f\"- Z-stack: {size_z}\")\n",
    "        print(f\"- Channels: {size_c}\")\n",
    "        print(f\"- Timepoints: {size_t}\")\n",
    "\n",
    "# Validate that data_id matches datatype and print details\n",
    "if datatype == \"project\":\n",
    "    project = conn.getObject(\"Project\", data_id)\n",
    "    if project is None:\n",
    "        raise ValueError(f\"Project with ID {data_id} not found\")\n",
    "    print_object_details(conn, project, \"project\")\n",
    "    \n",
    "elif datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    if plate is None:\n",
    "        raise ValueError(f\"Plate with ID {data_id} not found\")\n",
    "    print_object_details(conn, plate, \"plate\")\n",
    "    \n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    if dataset is None:\n",
    "        raise ValueError(f\"Dataset with ID {data_id} not found\")\n",
    "    print_object_details(conn, dataset, \"dataset\")\n",
    "    \n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image with ID {data_id} not found\")\n",
    "    print_object_details(conn, image, \"image\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid datatype specified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stardist on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get imageIDs to process\n",
    "- make sure that all images in your Project/dataset/plate are same type images e.g. time series, z-stacks and or multichannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for development of the src.ProcessImage module\n",
    "import importlib\n",
    "import src.ProcessImage as ProcessImage\n",
    "importlib.reload(ProcessImage)\n",
    "\n",
    "# Get list of images to process based on datatype\n",
    "if datatype == \"plate\":\n",
    "    images_ids = ezomero.get_image_ids(conn, plate=data_id)\n",
    "    images = [conn.getObject(\"Image\", id) for id in images_ids]\n",
    "    print(f\"Processing {len(images)} images from plate {data_id}\")\n",
    "elif datatype == \"dataset\":\n",
    "    images_ids = ezomero.get_image_ids(conn, dataset=data_id)\n",
    "    images = [conn.getObject(\"Image\", id) for id in images_ids]\n",
    "    print(f\"Processing {len(images)} images from dataset {data_id}\")\n",
    "elif datatype == \"image\":\n",
    "    images = [conn.getObject(\"Image\", data_id)]\n",
    "    print(f\"Processing 1 image with ID {data_id}\")\n",
    "elif datatype == \"project\":\n",
    "    images_ids = ezomero.get_image_ids(conn, project=data_id)\n",
    "    images = [conn.getObject(\"Image\", id) for id in images_ids]\n",
    "    print(f\"Processing {len(images)} images from project {data_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_statistics = []\n",
    "for count,image in enumerate(images):\n",
    "    print(f'Processing image {count+1}/{len(images)}: {image.getName()}')\n",
    "    \n",
    "    # Initialize processing\n",
    "    img = ProcessImage.ProcessImage(conn, image, job_id, model)\n",
    "    \n",
    "    # Segment nuclei\n",
    "    img.segment_nuclei(nucl_channel)\n",
    "    \n",
    "    # Save results\n",
    "    img.save_segmentation_to_omero_as_attach(new_output_directory) \n",
    "    img.save_segmentation_to_omero_as_roi()\n",
    "    #img.save_segmentation_to_omero_as_new_image(seg_img_name,desc)\n",
    "    \n",
    "    # Measure intensity\n",
    "    img.measure_intensity(norm=False)\n",
    "    all_statistics_df = img.get_measurements_to_df()\n",
    "    \n",
    "    #save intensity measurements to OMERO\n",
    "    all_statistics_df['imageID'] = image.getId()\n",
    "\n",
    "    plate_statistics.append(all_statistics_df)\n",
    "    image_id = image.getId()\n",
    "    tabelid = ezomero.post_table(conn, object_type=\"Image\", object_id=image.getId(), table = all_statistics_df,title=f\"Nuclei_measurements_{job_id}_{image_id}\")\n",
    "    print('Created table ID:', tabelid)\n",
    "       \n",
    "# Concatenate all statistics into a single DataFrame\n",
    "plate_statistics_df = pd.concat(plate_statistics, ignore_index=True)\n",
    "tabelid = ezomero.post_table(conn, object_type=\"Dataset\", object_id=data_id, table = plate_statistics_df, title=f\"Nuclei_measurements_{job_id}_{data_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore Stardist segmentation on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_statistics = []\n",
    "#for count,image in enumerate(images):\n",
    "image_number = 0\n",
    "image = images[image_number]\n",
    "count = image_number\n",
    "print(f'Processing image {count+1}/{len(images)}: {image.getName()}')\n",
    "\n",
    "# Initialize processing\n",
    "img = ProcessImage.ProcessImage(conn, image, job_id, model)\n",
    "\n",
    "# Segment nuclei\n",
    "img.segment_nuclei(nucl_channel)\n",
    "\n",
    "# Save results\n",
    "#img.save_segmentation_to_omero_as_attach(new_output_directory) \n",
    "#img.save_segmentation_to_omero_as_roi()\n",
    "#img.save_segmentation_to_omero_as_new_image(seg_img_name,desc)\n",
    "\n",
    "# Measure intensity\n",
    "img.measure_intensity(norm=False)\n",
    "all_statistics_df = img.get_measurements_to_df()\n",
    "#save intensity measurements to OMERO\n",
    "#all_statistics_df['imageID'] = image.getId()\n",
    "\n",
    "# image_id = image.getId()\n",
    "# tabelid = ezomero.post_table(conn, object_type=\"Image\", object_id=image.getId(), table = all_statistics_df,title=f\"Nuclei_measurements_{job_id}_{image_id}\")\n",
    "# print('Created table ID:', tabelid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img.measure_intensity(norm=False)\n",
    "#all_statistics_df = img.get_measurements_to_df()\n",
    "#img.visualize_measurements(nucl_channel)\n",
    "import stackview\n",
    "np.shape(img.labels)\n",
    "labels = img.labels\n",
    "print(np.shape(labels))\n",
    "import pyclesperanto_prototype as cle\n",
    "\n",
    "image = img.get_image_stack()\n",
    "#stackview.slice(image, continuous_update=True)\n",
    "\n",
    "\n",
    "#stackview.curtain(image[:,0,:,:], img.labels)\n",
    "\n",
    "plt = stackview.imshow(image[3,0,:,:], continue_drawing=True)\n",
    "stackview.imshow(np.array(labels)[3,:,:], plot=plt, alpha=0.4, title='image + labels')\n",
    "\n",
    "import tifffile\n",
    "tifffile.imwrite('image.tif', image)\n",
    "tifffile.imwrite('labels.tif', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/tables.ipynb\n",
    "import numpy as np\n",
    "import napari\n",
    "import pandas\n",
    "from napari_skimage_regionprops import regionprops_table, add_table, get_table\n",
    "\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image[3,0,:,:])\n",
    "viewer.add_labels(np.array(labels)[3,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete attachements from project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = \t502\n",
    "\n",
    "def ensure_list(obj):\n",
    "    if not obj:\n",
    "        return []\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    return [obj]\n",
    "\n",
    "if datatype == \"dataset\":\n",
    "    images = list(dataset.listChildren())\n",
    "    image_count = len(images)\n",
    "    plate_statistics = []\n",
    "    to_delete = []\n",
    "    for count in range(image_count):\n",
    "        image = images[count]\n",
    "        i = conn.getObject(\"Image\", image.getId())\n",
    "        print('Image Name:', i.getName())\n",
    "        \n",
    "        for ann in i.listAnnotations():\n",
    "            link_id = ann.link.id  # sometimes single, sometimes list\n",
    "            link_ids = ensure_list(link_id)\n",
    "\n",
    "            for lid in link_ids:\n",
    "                to_delete.append(lid)\n",
    "    conn.deleteObjects(\"ImageAnnotationLink\", to_delete, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete ROIs from project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = \t502\n",
    "\n",
    "if datatype == \"dataset\":\n",
    "    images = list(dataset.listChildren())\n",
    "    image_count = len(images)\n",
    "    plate_statistics = []\n",
    "    for count in range(image_count):\n",
    "        image = images[count]\n",
    "        roi_service = conn.getRoiService()\n",
    "        result = roi_service.findByImage(image.getId(), None)\n",
    "        roi_ids = [roi.id.val for roi in result.rois]\n",
    "        conn.deleteObjects(\"Roi\", roi_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
