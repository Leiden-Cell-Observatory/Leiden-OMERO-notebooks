{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Stardist segmentation on 2D/3D/timelapse OMERO images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for Stardist segmentation. Some inspiration from the https://github.com/ome/omero-guide-cellprofiler/idr0002.ipynb\n",
    "\n",
    "## TO DO\n",
    "- Make a generic function for 2D segmentation for all slices independent of the shape of the image z,c,t\n",
    "- Include a ID to all files uploaded to OMERO to make it more tracable\n",
    "- Extend to handle multiple channels AND timepoints\n",
    "- Check if we can overwrite label images if nesseary or ROIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OMERO Python BlitzGateway\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import Python System Packages\n",
    "import os\n",
    "import tempfile\n",
    "import pandas\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#stardist related\n",
    "from stardist.models import StarDist2D\n",
    "from csbdeep.utils import normalize\n",
    "from stardist.plot import render_label\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "\n",
    "#load stardist model\n",
    "model = StarDist2D.from_pretrained('2D_versatile_fluo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Temp Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "new_output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print(new_output_directory)\n",
    "#create unique job id for reference based on date and time\n",
    "job_id = str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "conn = BlitzGateway(host=os.environ.get(\"HOST\"), username=os.environ.get(\"USER_NAME\"), passwd=os.environ.get(\"PASSWORD\"), secure=True)\n",
    "print(conn.connect())\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = \t502\n",
    "nucl_channel = 0\n",
    "\n",
    "#validate that data_id matches datatype\n",
    "if datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    print('Plate Name: ', plate.getName())\n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    print('Dataset Name: ', dataset.getName())\n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    print('Image Name: ', image.getName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stardist on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get imageIDs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.ProcessImage as ProcessImage\n",
    "importlib.reload(ProcessImage)\n",
    "import pandas as pd\n",
    "\n",
    "#get list of images to process\n",
    "if datatype == \"plate\":\n",
    "    wells = list(plate.listChildren())\n",
    "    well_count = len(wells)\n",
    "    images = []\n",
    "    for count, well in enumerate(wells):\n",
    "        print('Well: %s/%s' % (count + 1, well_count), 'row:', well.row, 'column:', well.column)\n",
    "        # Load all images for a well if there are multiple\n",
    "        fields = well.countWellSample()\n",
    "        for field in range(fields):\n",
    "            print('Field:', field)\n",
    "            image = well.getImage(field)\n",
    "            images.append(image)\n",
    "elif datatype == \"dataset\":\n",
    "    images = list(dataset.listChildren())\n",
    "elif datatype == \"image\":\n",
    "    images = [image]   \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_statistics = []\n",
    "for count,image in enumerate(images):\n",
    "    print(f'Processing image {count+1}/{len(images)}: {image.getName()}')\n",
    "    \n",
    "    # Initialize processing\n",
    "    img = ProcessImage.ProcessImage(conn, image, job_id, model)\n",
    "    \n",
    "    # Segment nuclei\n",
    "    img.segment_nuclei(nucl_channel)\n",
    "    \n",
    "    # Save results\n",
    "    img.save_segmentation_to_omero_as_attach(new_output_directory) \n",
    "    img.save_segmentation_to_omero_as_roi()\n",
    "    #img.save_segmentation_to_omero_as_new_image(seg_img_name,desc)\n",
    "    \n",
    "    # Measure intensity\n",
    "    img.measure_intensity(norm=False)\n",
    "    all_statistics_df = img.get_measurements_to_df()\n",
    "    \n",
    "    #save intensity measurements to OMERO\n",
    "    all_statistics_df['imageID'] = image.getId()\n",
    "\n",
    "    plate_statistics.append(all_statistics_df)\n",
    "    image_id = image.getId()\n",
    "    tabelid = ezomero.post_table(conn, object_type=\"Image\", object_id=image.getId(), table = all_statistics_df,title=f\"Nuclei_measurements_{job_id}_{image_id}\")\n",
    "    print('Created table ID:', tabelid)\n",
    "       \n",
    "# Concatenate all statistics into a single DataFrame\n",
    "plate_statistics_df = pd.concat(plate_statistics, ignore_index=True)\n",
    "tabelid = ezomero.post_table(conn, object_type=\"Dataset\", object_id=data_id, table = plate_statistics_df, title=f\"Nuclei_measurements_{job_id}_{data_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore results on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_statistics = []\n",
    "#for count,image in enumerate(images):\n",
    "image_number = 0\n",
    "image = images[image_number]\n",
    "count = image_number\n",
    "print(f'Processing image {count+1}/{len(images)}: {image.getName()}')\n",
    "\n",
    "# Initialize processing\n",
    "img = ProcessImage.ProcessImage(conn, image, job_id, model)\n",
    "\n",
    "# Segment nuclei\n",
    "img.segment_nuclei(nucl_channel)\n",
    "\n",
    "# Save results\n",
    "#img.save_segmentation_to_omero_as_attach(new_output_directory) \n",
    "#img.save_segmentation_to_omero_as_roi()\n",
    "#img.save_segmentation_to_omero_as_new_image(seg_img_name,desc)\n",
    "\n",
    "# Measure intensity\n",
    "img.measure_intensity(norm=False)\n",
    "all_statistics_df = img.get_measurements_to_df()\n",
    "#save intensity measurements to OMERO\n",
    "#all_statistics_df['imageID'] = image.getId()\n",
    "\n",
    "# image_id = image.getId()\n",
    "# tabelid = ezomero.post_table(conn, object_type=\"Image\", object_id=image.getId(), table = all_statistics_df,title=f\"Nuclei_measurements_{job_id}_{image_id}\")\n",
    "# print('Created table ID:', tabelid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img.measure_intensity(norm=False)\n",
    "#all_statistics_df = img.get_measurements_to_df()\n",
    "#img.visualize_measurements(nucl_channel)\n",
    "import stackview\n",
    "np.shape(img.labels)\n",
    "labels = img.labels\n",
    "print(np.shape(labels))\n",
    "import pyclesperanto_prototype as cle\n",
    "\n",
    "image = img.get_image_stack()\n",
    "#stackview.slice(image, continuous_update=True)\n",
    "\n",
    "\n",
    "#stackview.curtain(image[:,0,:,:], img.labels)\n",
    "\n",
    "plt = stackview.imshow(image[3,0,:,:], continue_drawing=True)\n",
    "stackview.imshow(np.array(labels)[3,:,:], plot=plt, alpha=0.4, title='image + labels')\n",
    "\n",
    "import tifffile\n",
    "tifffile.imwrite('image.tif', image)\n",
    "tifffile.imwrite('labels.tif', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/haesleinhuepf/napari-skimage-regionprops/blob/master/demo/tables.ipynb\n",
    "import numpy as np\n",
    "import napari\n",
    "import pandas\n",
    "from napari_skimage_regionprops import regionprops_table, add_table, get_table\n",
    "\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image[3,0,:,:])\n",
    "viewer.add_labels(np.array(labels)[3,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete attachements from project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = \t502\n",
    "\n",
    "def ensure_list(obj):\n",
    "    if not obj:\n",
    "        return []\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    return [obj]\n",
    "\n",
    "if datatype == \"dataset\":\n",
    "    images = list(dataset.listChildren())\n",
    "    image_count = len(images)\n",
    "    plate_statistics = []\n",
    "    to_delete = []\n",
    "    for count in range(image_count):\n",
    "        image = images[count]\n",
    "        i = conn.getObject(\"Image\", image.getId())\n",
    "        print('Image Name:', i.getName())\n",
    "        \n",
    "        for ann in i.listAnnotations():\n",
    "            link_id = ann.link.id  # sometimes single, sometimes list\n",
    "            link_ids = ensure_list(link_id)\n",
    "\n",
    "            for lid in link_ids:\n",
    "                to_delete.append(lid)\n",
    "    conn.deleteObjects(\"ImageAnnotationLink\", to_delete, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete ROIs from project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = \t502\n",
    "\n",
    "if datatype == \"dataset\":\n",
    "    images = list(dataset.listChildren())\n",
    "    image_count = len(images)\n",
    "    plate_statistics = []\n",
    "    for count in range(image_count):\n",
    "        image = images[count]\n",
    "        roi_service = conn.getRoiService()\n",
    "        result = roi_service.findByImage(image.getId(), None)\n",
    "        roi_ids = [roi.id.val for roi in result.rois]\n",
    "        conn.deleteObjects(\"Roi\", roi_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
