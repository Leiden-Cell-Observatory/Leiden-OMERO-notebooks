{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning SAM with OMERO data using a batch approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- make it work for different OMERO data types (single images, plates, etc)\n",
    "- store all annotations into OMERO, see: https://github.com/computational-cell-analytics/micro-sam/issues/445; in series annotator possible to add commit path with prompts, but they get overwritten\n",
    "- clean up the errors and warnings output from napari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "  - To make it easier to run with OMERO and to not expose login and passwords password is stored in .env file (see example .env_example) . Still it is not recommended to save credentials unencrypted hence a better solution will be worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMERO-related imports\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "\n",
    "# Scientific computing and image processing\n",
    "import cv2\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File and system operations\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "import warnings\n",
    "from tifffile import imwrite\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Dask and Zarr for lazy loading and processing\n",
    "import dask\n",
    "import dask.array as da\n",
    "import zarr\n",
    "\n",
    "# Micro-SAM and Napari\n",
    "from napari.settings import get_settings\n",
    "from micro_sam.sam_annotator import image_series_annotator, annotator_2d\n",
    "from micro_sam.util import precompute_image_embeddings\n",
    "import napari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "conn = BlitzGateway(host=os.environ.get(\"HOST\"), username=os.environ.get(\"USER_NAME\"), passwd=os.environ.get(\"PASSWORD\"), group=os.environ.get(\"GROUP\"), secure=True)\n",
    "connection_status = conn.connect()\n",
    "if connection_status:\n",
    "    print(\"Connected to OMERO Server\")\n",
    "else:\n",
    "    print(\"Connection to OMERO Server Failed\")\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = 1112\n",
    "nucl_channel = 0\n",
    "\n",
    "def print_object_details(conn, obj, datatype):\n",
    "    \"\"\"Print detailed information about OMERO objects\"\"\"\n",
    "    print(f\"\\n{datatype.capitalize()} Details:\")\n",
    "    print(f\"- Name: {obj.getName()}\")\n",
    "    print(f\"- ID: {obj.getId()}\")\n",
    "    print(f\"- Owner: {obj.getOwner().getFullName()}\")\n",
    "    print(f\"- Group: {obj.getDetails().getGroup().getName()}\")\n",
    "    \n",
    "    if datatype == \"project\":\n",
    "        datasets = list(obj.listChildren())\n",
    "        dataset_count = len(datasets)\n",
    "        total_images = sum(len(list(ds.listChildren())) for ds in datasets)\n",
    "        print(f\"- Number of datasets: {dataset_count}\")\n",
    "        print(f\"- Total images: {total_images}\")\n",
    "        \n",
    "    elif datatype == \"plate\":\n",
    "        wells = list(obj.listChildren())\n",
    "        well_count = len(wells)\n",
    "        print(f\"- Number of wells: {well_count}\")\n",
    "        \n",
    "    elif datatype == \"dataset\":\n",
    "        images = list(obj.listChildren())\n",
    "        image_count = len(images)\n",
    "        # Get project info if dataset is in a project\n",
    "        projects = obj.getParent()\n",
    "        if projects:\n",
    "            print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Project: None (orphaned dataset)\")\n",
    "        print(f\"- Number of images: {image_count}\")\n",
    "        \n",
    "    elif datatype == \"image\":\n",
    "        size_x = obj.getSizeX()\n",
    "        size_y = obj.getSizeY()\n",
    "        size_z = obj.getSizeZ()\n",
    "        size_c = obj.getSizeC()\n",
    "        size_t = obj.getSizeT()\n",
    "        # Get dataset info if image is in a dataset\n",
    "        datasets = obj.getParent()\n",
    "        if datasets:\n",
    "            print(f\"- Dataset: {datasets.getName()} (ID: {datasets.getId()})\")\n",
    "            # Get project info if dataset is in a project\n",
    "            projects = datasets.getParent()\n",
    "            if projects:\n",
    "                print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Dataset: None (orphaned image)\")\n",
    "        print(f\"- Dimensions: {size_x}x{size_y}\")\n",
    "        print(f\"- Z-stack: {size_z}\")\n",
    "        print(f\"- Channels: {size_c}\")\n",
    "        print(f\"- Timepoints: {size_t}\")\n",
    "\n",
    "# Validate that data_id matches datatype and print details\n",
    "if datatype == \"project\":\n",
    "    project = conn.getObject(\"Project\", data_id)\n",
    "    if project is None:\n",
    "        raise ValueError(f\"Project with ID {data_id} not found\")\n",
    "    print_object_details(conn, project, \"project\")\n",
    "    \n",
    "elif datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    if plate is None:\n",
    "        raise ValueError(f\"Plate with ID {data_id} not found\")\n",
    "    print_object_details(conn, plate, \"plate\")\n",
    "    \n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    if dataset is None:\n",
    "        raise ValueError(f\"Dataset with ID {data_id} not found\")\n",
    "    print_object_details(conn, dataset, \"dataset\")\n",
    "    \n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image with ID {data_id} not found\")\n",
    "    print_object_details(conn, image, \"image\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid datatype specified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temporary folder to store training data, this will be uploaded to OMERO later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print('Output Directory: ', output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_directory(source_path, zarr_path, zip_file):\n",
    "    \"\"\"Zip a directory while handling null characters in paths.\"\"\"\n",
    "    for root, dirs, files in os.walk(zarr_path):\n",
    "        for file in files:\n",
    "            try:\n",
    "                # Create paths\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, source_path)\n",
    "                \n",
    "                # Remove null characters while preserving the path structure\n",
    "                safe_full_path = full_path.replace('\\x00', '')\n",
    "                safe_rel_path = rel_path.replace('\\x00', '')\n",
    "                \n",
    "                # Add file to zip if it exists\n",
    "                if os.path.exists(safe_full_path):\n",
    "                    zip_file.write(safe_full_path, safe_rel_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "def interleave_arrays(train_images, validate_images):\n",
    "    \"\"\"\n",
    "    Interleave two arrays of images in the pattern: train[0], validate[0], train[1], validate[1], ...\n",
    "    If arrays are of unequal length, remaining elements are appended at the end.\n",
    "    \"\"\"\n",
    "    # Create empty list to store interleaved images\n",
    "    interleaved = []\n",
    "    sequence = []\n",
    "    # Get the length of the longer array\n",
    "    max_len = max(len(train_images), len(validate_images))\n",
    "    \n",
    "    # Interleave the arrays\n",
    "    for i in range(max_len):\n",
    "        # Add train image if available\n",
    "        if i < len(train_images):\n",
    "            interleaved.append(train_images[i])\n",
    "            sequence.append(0)\n",
    "        # Add validate image if available\n",
    "        if i < len(validate_images):\n",
    "            interleaved.append(validate_images[i])\n",
    "            sequence.append(1)\n",
    "    \n",
    "    return np.array(interleaved), np.array(sequence)\n",
    "\n",
    "\n",
    "def mask_to_contour(mask):\n",
    "    \"\"\"Converts a binary mask to a list of ROI coordinates.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): binary mask\n",
    "\n",
    "    Returns:\n",
    "        list: list of ROI coordinates\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "def label_to_rois(label_img, z_slice, channel, timepoint, model_type):\n",
    "    \"\"\"\n",
    "    Convert a 2D label image to OMERO ROI shapes\n",
    "    \n",
    "    Args:\n",
    "        label_img (np.ndarray): 2D labeled image\n",
    "        z_slice (int): Z-slice index\n",
    "        channel (int): Channel index\n",
    "        timepoint (int): Time point index\n",
    "        model_type (str): SAM model type used\n",
    "    \n",
    "    Returns:\n",
    "        list: List of OMERO shape objects\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    unique_labels = np.unique(label_img)\n",
    "    \n",
    "    # Skip background (label 0)\n",
    "    for label in unique_labels[1:]:\n",
    "        # Create binary mask for this label\n",
    "        mask = (label_img == label).astype(np.uint8)\n",
    "        \n",
    "        # Get contours\n",
    "        contours = mask_to_contour(mask)\n",
    "        \n",
    "        # Convert each contour to polygon ROI\n",
    "        for contour in contours:\n",
    "            contour = contour[:, 0, :]  # Reshape to (N, 2)\n",
    "            # Create polygon without text parameter\n",
    "            poly = ezomero.rois.Polygon(\n",
    "                points=contour,  # explicitly name the points parameter\n",
    "                z=z_slice,\n",
    "                c=channel,\n",
    "                t=timepoint,\n",
    "                label=f'micro_sam.manual_instance_segmentation.{model_type}'\n",
    "            )\n",
    "            shapes.append(poly)\n",
    "    \n",
    "    return shapes\n",
    "\n",
    "#TODO merge these functions to a source file with functions for processing OMERO data\n",
    "def upload_rois_and_labels(conn, image, label_file, z_slice, channel, timepoint, model_type):\n",
    "    \"\"\"\n",
    "    Upload both label map and ROIs for a segmented image\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        image: OMERO image object\n",
    "        label_file: Path to the label image file\n",
    "        z_slice: Z-slice index\n",
    "        channel: Channel index\n",
    "        timepoint: Time point index\n",
    "        model_type: SAM model type used\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (label_id, roi_id)\n",
    "    \"\"\"\n",
    "    # Upload label map as attachment\n",
    "    label_id = ezomero.post_file_annotation(\n",
    "        conn,\n",
    "        str(label_file),\n",
    "        ns='microsam.labelimage',\n",
    "        object_type=\"Image\",\n",
    "        object_id=image.getId(),\n",
    "        description=f'SAM manual segmentation ({model_type})'\n",
    "    )\n",
    "    \n",
    "    # Create ROIs from label image\n",
    "    label_img = imageio.imread(label_file)\n",
    "    shapes = label_to_rois(label_img, z_slice, channel, timepoint, model_type)\n",
    "    \n",
    "    if shapes:  # Only create ROI if shapes were found\n",
    "        roi_id = ezomero.post_roi(\n",
    "            conn,\n",
    "            image.getId(),\n",
    "            shapes,\n",
    "            name=f'SAM_{model_type}',\n",
    "            description=f'micro_sam.manual_instance_segmentation.{model_type}'\n",
    "        )\n",
    "    else:\n",
    "        roi_id = None\n",
    "        \n",
    "    return label_id, roi_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Lazy Loading Functions for OMERO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dask_image(conn, image_id, z_slice=None, timepoint=None, channel=None):\n",
    "    \"\"\"\n",
    "    Get a dask array representation of an OMERO image for lazy loading\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        image_id: ID of image to load\n",
    "        z_slice: Optional specific Z slice to load (int or list)\n",
    "        timepoint: Optional specific timepoint to load (int or list)\n",
    "        channel: Optional specific channel to load (int or list)\n",
    "    \n",
    "    Returns:\n",
    "        dask array representation of image\n",
    "    \"\"\"\n",
    "    image = conn.getObject(\"Image\", image_id)\n",
    "    pixels = image.getPrimaryPixels()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    size_z = image.getSizeZ()\n",
    "    size_c = image.getSizeC()\n",
    "    size_t = image.getSizeT()\n",
    "    size_y = image.getSizeY()\n",
    "    size_x = image.getSizeX()\n",
    "    \n",
    "    # Define specific dimensions to load if provided\n",
    "    z_range = [z_slice] if isinstance(z_slice, int) else (range(size_z) if z_slice is None else z_slice)\n",
    "    t_range = [timepoint] if isinstance(timepoint, int) else (range(size_t) if timepoint is None else timepoint)\n",
    "    c_range = [channel] if isinstance(channel, int) else (range(size_c) if channel is None else channel)\n",
    "    \n",
    "    # Create empty list to store delayed objects\n",
    "    delayed_planes = {}\n",
    "    \n",
    "    print(f\"Creating dask array for image {image_id} with lazy loading\")\n",
    "    print(f\"Dimensions: Z={len(z_range)}, C={len(c_range)}, T={len(t_range)}, Y={size_y}, X={size_x}\")\n",
    "    \n",
    "    # Create lazy loading function\n",
    "    @dask.delayed\n",
    "    def get_plane(z, c, t):\n",
    "        print(f\"Loading plane: Z={z}, C={c}, T={t}\")\n",
    "        return pixels.getPlane(z, c, t)\n",
    "    \n",
    "    # Build dask arrays\n",
    "    arrays = []\n",
    "    for t in t_range:\n",
    "        t_arrays = []\n",
    "        for z in z_range:\n",
    "            z_arrays = []\n",
    "            for c in c_range:\n",
    "                # Create a key for this plane\n",
    "                key = (z, c, t)\n",
    "                \n",
    "                # Check if we've already created this delayed object\n",
    "                if key not in delayed_planes:\n",
    "                    # Create a delayed object for this plane\n",
    "                    delayed_plane = get_plane(z, c, t)\n",
    "                    delayed_planes[key] = delayed_plane\n",
    "                else:\n",
    "                    delayed_plane = delayed_planes[key]\n",
    "                \n",
    "                # Convert to dask array with known shape and dtype\n",
    "                shape = (size_y, size_x)\n",
    "                dtype = np.uint16  # Most OMERO images use 16-bit\n",
    "                dask_plane = da.from_delayed(delayed_plane, shape=shape, dtype=dtype)\n",
    "                z_arrays.append(dask_plane)\n",
    "            if z_arrays:\n",
    "                # Stack channels for this z position\n",
    "                t_arrays.append(da.stack(z_arrays))\n",
    "        if t_arrays:\n",
    "            # Stack z-planes for this timepoint\n",
    "            arrays.append(da.stack(t_arrays))\n",
    "    \n",
    "    if arrays:\n",
    "        # Stack all timepoints\n",
    "        return da.stack(arrays)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def store_annotations_in_zarr(mask_data, output_folder, image_num):\n",
    "    \"\"\"\n",
    "    Store annotation masks in zarr format for efficient access\n",
    "    \n",
    "    Args:\n",
    "        mask_data: Numpy array with mask data\n",
    "        output_folder: Base folder to store zarr data\n",
    "        image_num: Image number/identifier\n",
    "        \n",
    "    Returns:\n",
    "        path: Path to the zarr store\n",
    "    \"\"\"\n",
    "    # Create zarr directory if it doesn't exist\n",
    "    zarr_dir = os.path.join(output_folder, \"annotations\")\n",
    "    os.makedirs(zarr_dir, exist_ok=True)\n",
    "    \n",
    "    # Create zarr filename\n",
    "    zarr_path = os.path.join(zarr_dir, f\"annotation_{image_num:05d}.zarr\")\n",
    "    \n",
    "    # Remove existing zarr store if it exists\n",
    "    if os.path.exists(zarr_path):\n",
    "        shutil.rmtree(zarr_path)\n",
    "        \n",
    "    # Create zarr array from mask data\n",
    "    z = zarr.open(zarr_path, mode='w')\n",
    "    z.create_dataset('masks', data=mask_data, chunks=(256, 256))\n",
    "    \n",
    "    # Return path to zarr store\n",
    "    return zarr_path\n",
    "\n",
    "def zarr_to_tiff(zarr_path, output_tiff_path):\n",
    "    \"\"\"\n",
    "    Convert zarr store to TIFF file for OMERO upload\n",
    "    \n",
    "    Args:\n",
    "        zarr_path: Path to zarr store\n",
    "        output_tiff_path: Path to save TIFF file\n",
    "        \n",
    "    Returns:\n",
    "        output_tiff_path: Path to saved TIFF file\n",
    "    \"\"\"\n",
    "    # Load data from zarr\n",
    "    z = zarr.open(zarr_path, mode='r')\n",
    "    mask_data = z['masks'][:]\n",
    "    \n",
    "    # Save as TIFF\n",
    "    imwrite(output_tiff_path, mask_data)\n",
    "    \n",
    "    return output_tiff_path\n",
    "\n",
    "def cleanup_local_embeddings(output_folder):\n",
    "    \"\"\"\n",
    "    Check for and clean up any existing embeddings from previous interrupted runs\n",
    "    \n",
    "    Args:\n",
    "        output_folder: Path to the output folder containing embeddings\n",
    "    \"\"\"\n",
    "    embed_path = os.path.join(output_folder, \"embed\")\n",
    "    \n",
    "    if os.path.exists(embed_path):\n",
    "        # Look for embedding zarr directories and zip files\n",
    "        for item in os.listdir(embed_path):\n",
    "            item_path = os.path.join(embed_path, item)\n",
    "            if os.path.isdir(item_path) and \"embedding_\" in item and item.endswith(\".zarr\"):\n",
    "                print(f\"Cleaning up leftover embedding directory: {item}\")\n",
    "                shutil.rmtree(item_path)\n",
    "            elif os.path.isfile(item_path) and \"embedding_\" in item and item.endswith(\".zip\"):\n",
    "                print(f\"Cleaning up leftover embedding zip: {item}\")\n",
    "                os.remove(item_path)\n",
    "    \n",
    "    # Check output directory for segmentation files\n",
    "    output_path = os.path.join(output_folder, \"output\")\n",
    "    if os.path.exists(output_path):\n",
    "        for item in os.listdir(output_path):\n",
    "            item_path = os.path.join(output_path, item)\n",
    "            if os.path.isfile(item_path) and \"seg_\" in item and (item.endswith(\".tif\") or item.endswith(\".tiff\")):\n",
    "                print(f\"Cleaning up leftover segmentation file: {item}\")\n",
    "                os.remove(item_path)\n",
    "\n",
    "def process_omero_batch_with_dask(\n",
    "    dataset,\n",
    "    output_folder: str,\n",
    "    model_type: str = 'vit_l',\n",
    "    batch_size: int = 3,\n",
    "    channel: int = 0,\n",
    "    timepoint: int = 0,\n",
    "    z_slice: int = 0,\n",
    "    segment_all: bool = True,\n",
    "    train_n: int = 3,\n",
    "    validate_n: int = 3,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Process OMERO dataset in batches for SAM segmentation using dask for lazy loading\n",
    "    and zarr for temporary annotation storage\n",
    "    \n",
    "    Args:\n",
    "        dataset: OMERO dataset object\n",
    "        output_folder: Path to store temporary files\n",
    "        model_type: SAM model type\n",
    "        batch_size: Number of images to process at once\n",
    "        channel: Channel to segment\n",
    "        timepoint: Timepoint to process\n",
    "        z_slice: Z-slice to process\n",
    "        segment_all: segment all images in the dataset or only train/validate subset\n",
    "        train_n: Number of training images if not segment_all\n",
    "        validate_n: Number of validation images if not segment_all\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (table_id, combined_images)\n",
    "    \"\"\"\n",
    "    # Setup output directories\n",
    "    output_path = os.path.join(output_folder, \"output\")\n",
    "    embed_path = os.path.join(output_folder, \"embed\")\n",
    "    zarr_path = os.path.join(output_folder, \"zarr\")\n",
    "    \n",
    "    # Check for and clean up any existing embeddings from interrupted runs\n",
    "    cleanup_local_embeddings(output_folder)\n",
    "    \n",
    "    # Remove directories if they exist\n",
    "    for path in [output_path, embed_path, zarr_path]:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # Get images from dataset\n",
    "    images_dataset = list(dataset.listChildren())\n",
    "    \n",
    "    # Select images based on segment_all flag\n",
    "    if segment_all:\n",
    "        combined_images = images_dataset\n",
    "        combined_images_sequence = np.zeros(len(combined_images))  # All treated as training\n",
    "    else:\n",
    "        # Check if we have enough images\n",
    "        if len(images_dataset) < train_n + validate_n:\n",
    "            print(\"Not enough images in dataset for training and validation\")\n",
    "            raise ValueError(f\"Need at least {train_n + validate_n} images but found {len(images_dataset)}\")\n",
    "            \n",
    "        # Select random images for training and validation\n",
    "        train_images = np.random.choice(images_dataset, train_n, replace=False)\n",
    "        validate_images = np.random.choice([x for x in images_dataset if x not in train_images], validate_n, replace=False)\n",
    "        combined_images, combined_images_sequence = interleave_arrays(train_images, validate_images)\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(combined_images) + batch_size - 1) // batch_size\n",
    "    df = pd.DataFrame(columns=[\n",
    "        \"image_id\", \"image_name\", \"train\", \"validate\", \n",
    "        \"channel\", \"z_slice\", \"timepoint\", \"sam_model\", \"embed_id\", \"label_id\", \"roi_id\"\n",
    "    ])\n",
    "    \n",
    "    print(f\"Processing {len(combined_images)} images in {total_batches} batches\")\n",
    "    \n",
    "    # Process images in batches\n",
    "    for batch_idx in range(total_batches):\n",
    "        print(f\"\\nProcessing batch {batch_idx+1}/{total_batches}\")\n",
    "        \n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(combined_images))\n",
    "        batch_images = combined_images[start_idx:end_idx]\n",
    "        \n",
    "        # Load batch images as dask arrays for lazy loading\n",
    "        # Use the global conn variable that was already connected instead of trying to get it from dataset\n",
    "        images = []\n",
    "        dask_images = []\n",
    "        image_ids = []\n",
    "        \n",
    "        for image in batch_images:\n",
    "            image_ids.append(image.getId())\n",
    "            \n",
    "            # Create both standard numpy array and dask array\n",
    "            # Standard array is needed for current micro-sam compatibility\n",
    "            pixels = image.getPrimaryPixels()\n",
    "            img = pixels.getPlane(z_slice, channel, timepoint)\n",
    "            images.append(img)\n",
    "            \n",
    "            print(f\"Creating dask array for image {image.getId()}\")\n",
    "            dask_img = get_dask_image(conn, image.getId(), z_slice=z_slice, \n",
    "                                    timepoint=timepoint, channel=channel)\n",
    "            dask_images.append(dask_img)\n",
    "        \n",
    "        # Process batch with SAM using standard numpy arrays for now\n",
    "        # Note: In the future, micro-sam could be updated to work directly with dask\n",
    "        print(\"Starting napari viewer with SAM annotator\")\n",
    "        viewer = napari.Viewer()\n",
    "        \n",
    "        # Add image series annotator\n",
    "        image_series_annotator(\n",
    "            images, \n",
    "            model_type=model_type,\n",
    "            viewer=viewer,\n",
    "            embedding_path=os.path.join(output_folder, \"embed\"),\n",
    "            output_folder=os.path.join(output_folder, \"output\")\n",
    "        )\n",
    "        \n",
    "        napari.run()\n",
    "        print(\"Done annotating batch, storing results in zarr and uploading to OMERO\")\n",
    "        \n",
    "        # Process results for batch\n",
    "        for n, image in enumerate(batch_images):\n",
    "            local_n = n  # Index within current batch\n",
    "            global_n = start_idx + n  # Global index across all batches\n",
    "            \n",
    "            # Store segmentation mask in zarr before uploading to OMERO\n",
    "            seg_file_path = os.path.join(output_folder, \"output\", f\"seg_{local_n:05d}.tif\")\n",
    "            if not os.path.exists(seg_file_path):\n",
    "                print(f\"Warning: Segmentation file not found for image {image.getId()}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Read the segmentation mask\n",
    "            mask_data = imageio.imread(seg_file_path)\n",
    "            \n",
    "            # Store in zarr format for efficient processing\n",
    "            zarr_file_path = store_annotations_in_zarr(mask_data, zarr_path, global_n)\n",
    "            \n",
    "            # Store embedding in zarr format and zip for OMERO upload\n",
    "            embed_zarr = f\"embedding_{local_n:05d}.zarr\"\n",
    "            embed_dir = os.path.join(output_folder, \"embed\")\n",
    "            zip_path = os.path.join(embed_dir, f\"embedding_{global_n:05d}.zip\")\n",
    "            \n",
    "            # Check if the embedding directory exists before trying to zip it\n",
    "            embed_zarr_path = os.path.join(embed_dir, embed_zarr)\n",
    "            if not os.path.exists(embed_zarr_path):\n",
    "                print(f\"Warning: Embedding directory {embed_zarr} not found, skipping embedding upload\")\n",
    "                embed_id = None\n",
    "            else:\n",
    "                with zipfile.ZipFile(zip_path, 'w') as zip_file:\n",
    "                    zip_directory(embed_dir, embed_zarr, zip_file)\n",
    "                \n",
    "                # Upload embedding to OMERO\n",
    "                embed_id = ezomero.post_file_annotation(\n",
    "                    conn,\n",
    "                    str(zip_path),\n",
    "                    ns='microsam.embeddings',\n",
    "                    object_type=\"Image\",\n",
    "                    object_id=image.getId(),\n",
    "                    description=f'SAM embedding ({model_type})'\n",
    "                )\n",
    "            \n",
    "            # Convert zarr annotation to TIFF for OMERO compatibility\n",
    "            tiff_path = os.path.join(output_folder, \"output\", f\"seg_{global_n:05d}.tiff\")\n",
    "            zarr_to_tiff(zarr_file_path, tiff_path)\n",
    "            \n",
    "            # Upload labels and create ROIs\n",
    "            label_id, roi_id = upload_rois_and_labels(\n",
    "                conn, \n",
    "                image, \n",
    "                tiff_path, \n",
    "                z_slice, \n",
    "                channel, \n",
    "                timepoint, \n",
    "                model_type\n",
    "            )\n",
    "            \n",
    "            # Update tracking dataframe\n",
    "            is_train = combined_images_sequence[global_n] == 0 if not segment_all else True\n",
    "            is_validate = combined_images_sequence[global_n] == 1 if not segment_all else False\n",
    "            \n",
    "            new_row = pd.DataFrame([{\n",
    "                \"image_id\": image.getId(),\n",
    "                \"image_name\": image.getName(),\n",
    "                \"train\": is_train,\n",
    "                \"validate\": is_validate,\n",
    "                \"channel\": channel,\n",
    "                \"z_slice\": z_slice,\n",
    "                \"timepoint\": timepoint,\n",
    "                \"sam_model\": model_type,\n",
    "                \"embed_id\": embed_id,\n",
    "                \"label_id\": label_id,\n",
    "                \"roi_id\": roi_id\n",
    "            }])\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            \n",
    "        # Clean up temporary files for this batch\n",
    "        for n in range(batch_size):  # Use local indexing for cleanup\n",
    "            if start_idx + n >= len(combined_images):  # Skip if we've processed all images\n",
    "                continue\n",
    "                \n",
    "            embed_zip = os.path.join(output_folder, \"embed\", f\"embedding_{n:05d}.zip\")\n",
    "            embed_zarr = os.path.join(output_folder, \"embed\", f\"embedding_{n:05d}.zarr\")\n",
    "            seg_file = os.path.join(output_folder, \"output\", f\"seg_{n:05d}.tif\")\n",
    "            \n",
    "            for path in [embed_zip, seg_file]:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    \n",
    "            if os.path.exists(embed_zarr) and os.path.isdir(embed_zarr):\n",
    "                shutil.rmtree(embed_zarr)\n",
    "    \n",
    "    # Upload final tracking table to OMERO\n",
    "    table_id = ezomero.post_table(\n",
    "        conn, \n",
    "        object_type=\"Dataset\", \n",
    "        object_id=dataset.getId(), \n",
    "        table=df,\n",
    "        title=\"micro_sam_training_data\"\n",
    "    )\n",
    "    \n",
    "    print(f\"All processing complete. Uploaded tracking table with ID: {table_id}\")\n",
    "    return table_id, combined_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from OMERO and open in napari with micro-sam annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running segmentation in batch\n",
    "\n",
    "Note: some warnings from napari are expected in the output here, generally not a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imput parameters\n",
    "model_type = 'vit_b'\n",
    "segment_all = False\n",
    "train_n = 20   \n",
    "validate_n = 20\n",
    "channel = 3 #which channel to segment starting from 0\n",
    "timepoint = 0\n",
    "z_slice = 4 #TODO for now pick one slice but add option to pick multiple slices by giving a list of z slices, or random slices\n",
    "batch_size = 10 # the number of images to process at once in napari\n",
    "\n",
    "# Configure napari settings\n",
    "settings = get_settings()\n",
    "settings.application.ipy_interactive = False\n",
    "\n",
    "# Run batch processing with dask lazy loading\n",
    "if datatype == \"dataset\":\n",
    "    table_id, processed_images = process_omero_batch_with_dask(\n",
    "        dataset=dataset,\n",
    "        output_folder=output_directory,\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        channel=channel,\n",
    "        timepoint=timepoint,\n",
    "        z_slice=z_slice,\n",
    "        segment_all=segment_all,\n",
    "        train_n=train_n,\n",
    "        validate_n=validate_n,\n",
    "    )\n",
    "    print(f\"Finished processing with dask lazy loading. Table ID: {table_id}\")\n",
    "    print(f\"Processed {len(processed_images)} images successfully.\")\n",
    "else:\n",
    "    print(f\"This notebook currently only supports processing dataset objects. Please set datatype to 'dataset'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
