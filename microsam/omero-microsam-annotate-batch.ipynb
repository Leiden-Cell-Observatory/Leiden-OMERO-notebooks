{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Fine tuning SAM with OMERO data using a batch approach - Enhanced Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Features\n",
    "- Supports multiple OMERO data types (single images, datasets, projects, plates, and screens)\n",
    "- Batch processing with micro-SAM for segmentation\n",
    "- Stores all annotations in OMERO as ROIs and attachments\n",
    "- Uses dask for lazy loading of images for better memory management\n",
    "- Supports 3D volumetric segmentation for z-stacks\n",
    "- **NEW**: Support for multiple z-slices in 2D mode\n",
    "- **NEW**: Support for time series analysis\n",
    "- **NEW**: Support for patch-based extraction and annotation\n",
    "- **NEW**: Improved resumption of annotation sessions\n",
    "\n",
    "### TODOs\n",
    "- Store all annotations into OMERO, see: https://github.com/computational-cell-analytics/micro-sam/issues/445; in series annotator possible to add commit path with prompts, but they get overwritten\n",
    "- Clean up the errors and warnings output from napari\n",
    "- Improve ROI creation for 3D volumes to better represent volumetric masks in OMERO\n",
    "- Work with Dask arrays directly in micro-sam\n",
    "- Add recovery mode to handle cases when users abort in the middle of a batch annotation session (currently annotations made before closing are preserved, but could be improved with a dedicated recovery workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "  - To make it easier to run with OMERO and to not expose login and passwords, password is stored in .env file (see example .env_example). Still it is not recommended to save credentials unencrypted hence a better solution will be worked on.\n",
    "  - This notebook supports processing images from various OMERO container types: images, datasets, projects, plates, and screens.\n",
    "  - Specify the container type in the `datatype` variable and the container ID in the `data_id` variable.\n",
    "  - You can choose to segment all images in the container or select a random subset for training and validation.\n",
    "  - **NEW**: You can now specify multiple z-slices and timepoints to analyze.\n",
    "  - **NEW**: You can extract and analyze patches from large images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMERO-related imports\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "\n",
    "# Scientific computing and image processing\n",
    "import cv2\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File and system operations\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "import warnings\n",
    "from tifffile import imwrite, imread\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Dask and Zarr for lazy loading and processing\n",
    "import dask\n",
    "import dask.array as da\n",
    "import zarr\n",
    "\n",
    "# Micro-SAM and Napari\n",
    "from napari.settings import get_settings\n",
    "from micro_sam.sam_annotator import image_series_annotator, annotator_2d\n",
    "from micro_sam.util import precompute_image_embeddings\n",
    "import napari\n",
    "\n",
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom encoder for numpy data types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "conn = BlitzGateway(host=os.environ.get(\"HOST\"), username=os.environ.get(\"USER_NAME\"), passwd=os.environ.get(\"PASSWORD\"), group=os.environ.get(\"GROUP\"), secure=True)\n",
    "connection_status = conn.connect()\n",
    "if connection_status:\n",
    "    print(\"Connected to OMERO Server\")\n",
    "else:\n",
    "    print(\"Connection to OMERO Server Failed\")\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"screen\", \"plate\", \"project\", \"dataset\", \"image\"\n",
    "data_id = 1112\n",
    "nucl_channel = 0\n",
    "\n",
    "def print_object_details(conn, obj, datatype):\n",
    "    \"\"\"Print detailed information about OMERO objects\"\"\"\n",
    "    print(f\"\\n{datatype.capitalize()} Details:\")\n",
    "    print(f\"- Name: {obj.getName()}\")\n",
    "    print(f\"- ID: {obj.getId()}\")\n",
    "    print(f\"- Owner: {obj.getOwner().getFullName()}\")\n",
    "    print(f\"- Group: {obj.getDetails().getGroup().getName()}\")\n",
    "    \n",
    "    if datatype == \"project\":\n",
    "        datasets = list(obj.listChildren())\n",
    "        dataset_count = len(datasets)\n",
    "        total_images = sum(len(list(ds.listChildren())) for ds in datasets)\n",
    "        print(f\"- Number of datasets: {dataset_count}\")\n",
    "        print(f\"- Total images: {total_images}\")\n",
    "        \n",
    "    elif datatype == \"plate\":\n",
    "        wells = list(obj.listChildren())\n",
    "        well_count = len(wells)\n",
    "        print(f\"- Number of wells: {well_count}\")\n",
    "        \n",
    "    elif datatype == \"dataset\":\n",
    "        images = list(obj.listChildren())\n",
    "        image_count = len(images)\n",
    "        # Get project info if dataset is in a project\n",
    "        projects = obj.getParent()\n",
    "        if projects:\n",
    "            print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Project: None (orphaned dataset)\")\n",
    "        print(f\"- Number of images: {image_count}\")\n",
    "        \n",
    "    elif datatype == \"image\":\n",
    "        size_x = obj.getSizeX()\n",
    "        size_y = obj.getSizeY()\n",
    "        size_z = obj.getSizeZ()\n",
    "        size_c = obj.getSizeC()\n",
    "        size_t = obj.getSizeT()\n",
    "        # Get dataset info if image is in a dataset\n",
    "        datasets = obj.getParent()\n",
    "        if datasets:\n",
    "            print(f\"- Dataset: {datasets.getName()} (ID: {datasets.getId()})\")\n",
    "            # Get project info if dataset is in a project\n",
    "            projects = datasets.getParent()\n",
    "            if projects:\n",
    "                print(f\"- Project: {projects.getName()} (ID: {projects.getId()})\")\n",
    "        else:\n",
    "            print(\"- Dataset: None (orphaned image)\")\n",
    "        print(f\"- Dimensions: {size_x}x{size_y}\")\n",
    "        print(f\"- Z-stack: {size_z}\")\n",
    "        print(f\"- Channels: {size_c}\")\n",
    "        print(f\"- Timepoints: {size_t}\")\n",
    "\n",
    "# Validate that data_id matches datatype and print details\n",
    "if datatype == \"project\":\n",
    "    project = conn.getObject(\"Project\", data_id)\n",
    "    if project is None:\n",
    "        raise ValueError(f\"Project with ID {data_id} not found\")\n",
    "    print_object_details(conn, project, \"project\")\n",
    "    \n",
    "elif datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    if plate is None:\n",
    "        raise ValueError(f\"Plate with ID {data_id} not found\")\n",
    "    print_object_details(conn, plate, \"plate\")\n",
    "    \n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    if dataset is None:\n",
    "        raise ValueError(f\"Dataset with ID {data_id} not found\")\n",
    "    print_object_details(conn, dataset, \"dataset\")\n",
    "    \n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image with ID {data_id} not found\")\n",
    "    print_object_details(conn, image, \"image\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid datatype specified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Create temporary folder to store training data, this will be uploaded to OMERO later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print('Output Directory: ', output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_directory(source_path, zarr_path, zip_file):\n",
    "    \"\"\"Zip a directory while handling null characters in paths.\"\"\"\n",
    "    for root, dirs, files in os.walk(zarr_path):\n",
    "        for file in files:\n",
    "            try:\n",
    "                # Create paths\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, source_path)\n",
    "                \n",
    "                # Remove null characters while preserving the path structure\n",
    "                safe_full_path = full_path.replace('\\x00', '')\n",
    "                safe_rel_path = rel_path.replace('\\x00', '')\n",
    "                \n",
    "                # Add file to zip if it exists\n",
    "                if os.path.exists(safe_full_path):\n",
    "                    zip_file.write(safe_full_path, safe_rel_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "def interleave_arrays(train_images, validate_images):\n",
    "    \"\"\"\n",
    "    Interleave two arrays of images in the pattern: train[0], validate[0], train[1], validate[1], ...\n",
    "    If arrays are of unequal length, remaining elements are appended at the end.\n",
    "    \"\"\"\n",
    "    # Create empty list to store interleaved images\n",
    "    interleaved = []\n",
    "    sequence = []\n",
    "    # Get the length of the longer array\n",
    "    max_len = max(len(train_images), len(validate_images))\n",
    "    \n",
    "    # Interleave the arrays\n",
    "    for i in range(max_len):\n",
    "        # Add train image if available\n",
    "        if i < len(train_images):\n",
    "            interleaved.append(train_images[i])\n",
    "            sequence.append(0)\n",
    "        # Add validate image if available\n",
    "        if i < len(validate_images):\n",
    "            interleaved.append(validate_images[i])\n",
    "            sequence.append(1)\n",
    "    \n",
    "    return np.array(interleaved), np.array(sequence)\n",
    "\n",
    "def mask_to_contour(mask):\n",
    "    \"\"\"Converts a binary mask to a list of ROI coordinates.\n",
    "\n",
    "    Args:\n",
    "        mask (np.ndarray): binary mask\n",
    "\n",
    "    Returns:\n",
    "        list: list of ROI coordinates\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "def label_to_rois(label_img, z_slice, channel, timepoint, model_type, is_volumetric=False, patch_offset=None):\n",
    "    \"\"\"\n",
    "    Convert a 2D or 3D label image to OMERO ROI shapes\n",
    "    \n",
    "    Args:\n",
    "        label_img (np.ndarray): 2D labeled image or 3D labeled stack\n",
    "        z_slice (int or list): Z-slice index or list/range of Z indices\n",
    "        channel (int): Channel index\n",
    "        timepoint (int): Time point index\n",
    "        model_type (str): SAM model type used\n",
    "        is_volumetric (bool): Whether the label image is 3D volumetric data\n",
    "        patch_offset: Optional (x,y) offset for placing ROIs in a larger image\n",
    "    \n",
    "    Returns:\n",
    "        list: List of OMERO shape objects\n",
    "    \"\"\"\n",
    "    shapes = []\n",
    "    \n",
    "    # Unpack patch offset if provided\n",
    "    x_offset, y_offset = (0, 0) if patch_offset is None else patch_offset\n",
    "    \n",
    "    if is_volumetric and label_img.ndim > 2:\n",
    "        # 3D volumetric data - process each z slice\n",
    "        for z_index, z_plane in enumerate(label_img):\n",
    "            # If z_slice is a range or list, use the actual z-index from that range\n",
    "            if isinstance(z_slice, (range, list)):\n",
    "                actual_z = z_slice[z_index] if z_index < len(z_slice) else z_slice[0] + z_index\n",
    "            else:\n",
    "                actual_z = z_slice + z_index  # Assume z_slice is the starting index\n",
    "                \n",
    "            print(f\"Processing volumetric ROIs for z-slice {actual_z}\")\n",
    "            shapes.extend(process_label_plane(z_plane, actual_z, channel, timepoint, model_type, \n",
    "                                            x_offset, y_offset))\n",
    "    else:\n",
    "        # 2D data - process single plane\n",
    "        shapes.extend(process_label_plane(label_img, z_slice, channel, timepoint, model_type, \n",
    "                                        x_offset, y_offset))\n",
    "    \n",
    "    return shapes\n",
    "\n",
    "def process_label_plane(label_plane, z_slice, channel, timepoint, model_type, x_offset=0, y_offset=0):\n",
    "    \"\"\"Process a single 2D label plane to generate OMERO shapes with optional offset\"\"\"\n",
    "    shapes = []\n",
    "    unique_labels = np.unique(label_plane)\n",
    "    \n",
    "    # Skip background (label 0)\n",
    "    for label in unique_labels[1:]:\n",
    "        # Create binary mask for this label\n",
    "        mask = (label_plane == label).astype(np.uint8)\n",
    "        \n",
    "        # Get contours\n",
    "        contours = mask_to_contour(mask)\n",
    "        \n",
    "        # Convert each contour to polygon ROI\n",
    "        for contour in contours:\n",
    "            contour = contour[:, 0, :]  # Reshape to (N, 2)\n",
    "            \n",
    "            # Apply offset to contour points if needed\n",
    "            if x_offset != 0 or y_offset != 0:\n",
    "                contour = contour + np.array([x_offset, y_offset])\n",
    "                \n",
    "            # Create polygon without text parameter\n",
    "            poly = ezomero.rois.Polygon(\n",
    "                points=contour,  # explicitly name the points parameter\n",
    "                z=z_slice,\n",
    "                c=channel,\n",
    "                t=timepoint,\n",
    "                label=f'micro_sam.{\"volumetric\" if isinstance(z_slice, (list, range)) or z_slice > 0 else \"manual\"}_instance_segmentation.{model_type}'\n",
    "            )\n",
    "            shapes.append(poly)\n",
    "    \n",
    "    return shapes\n",
    "\n",
    "def upload_rois_and_labels(conn, image, label_file, z_slice, channel, timepoint, model_type, \n",
    "                          is_volumetric=False, patch_offset=None, read_only_mode=False, local_output_dir=\"./omero_annotations\"):\n",
    "    \"\"\"\n",
    "    Upload both label map and ROIs for a segmented image or save them locally in read-only mode\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        image: OMERO image object\n",
    "        label_file: Path to the label image file\n",
    "        z_slice: Z-slice index or range of indices\n",
    "        channel: Channel index\n",
    "        timepoint: Time point index\n",
    "        model_type: SAM model type used\n",
    "        is_volumetric: Whether the data is 3D volumetric\n",
    "        patch_offset: Optional (x,y) offset for placing ROIs in a larger image\n",
    "        read_only_mode: If True, save annotations locally instead of uploading to OMERO\n",
    "        local_output_dir: Directory to save local annotations when in read-only mode\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (label_id, roi_id) or (local_label_path, local_roi_path) in read-only mode\n",
    "    \"\"\"\n",
    "    # Add patch info to description if applicable\n",
    "    patch_desc = \"\"\n",
    "    if patch_offset:\n",
    "        patch_desc = f\", Patch offset: ({patch_offset[0]}, {patch_offset[1]})\"\n",
    "    \n",
    "    # Create ROIs from label image\n",
    "    label_img = imageio.imread(label_file)\n",
    "    shapes = label_to_rois(label_img, z_slice, channel, timepoint, model_type, \n",
    "                          is_volumetric, patch_offset)\n",
    "    \n",
    "    if read_only_mode:\n",
    "        # Save annotations locally instead of uploading to OMERO\n",
    "        import json\n",
    "        import os\n",
    "        import shutil\n",
    "        \n",
    "        # Create local directories\n",
    "        image_id = image.getId()\n",
    "        image_dir = os.path.join(local_output_dir, f\"image_{image_id}\")\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        \n",
    "        # Save label image file\n",
    "        local_label_path = os.path.join(image_dir, os.path.basename(label_file))\n",
    "        shutil.copy2(label_file, local_label_path)\n",
    "        \n",
    "        # Save ROI data as JSON\n",
    "        local_roi_path = os.path.join(image_dir, f\"roi_{os.path.basename(label_file).split('.')[0]}.json\")\n",
    "        \n",
    "        # Prepare ROI metadata\n",
    "        roi_metadata = {\n",
    "            \"image_id\": image_id,\n",
    "            \"image_name\": image.getName(),\n",
    "            \"timestamp\": str(pd.Timestamp.now()),\n",
    "            \"model_type\": model_type,\n",
    "            \"is_volumetric\": is_volumetric,\n",
    "            \"z_slice\": z_slice if not isinstance(z_slice, range) else list(z_slice),\n",
    "            \"channel\": channel,\n",
    "            \"timepoint\": timepoint,\n",
    "            \"patch_offset\": patch_offset,\n",
    "            \"shapes_count\": len(shapes) if shapes else 0,\n",
    "            # We can't store the actual shapes because they're OMERO objects\n",
    "            # but we can save the label image which can be used to recreate them\n",
    "            \"label_image_path\": os.path.relpath(local_label_path, local_output_dir)\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(local_roi_path, 'w') as f:\n",
    "            json.dump(roi_metadata, f, indent=2, cls=NumpyEncoder)\n",
    "            \n",
    "        print(f\"Saved annotation locally in read-only mode to {image_dir}\")\n",
    "        return local_label_path, local_roi_path\n",
    "    else:\n",
    "        # Normal OMERO upload mode\n",
    "        # Upload label map as attachment\n",
    "        label_id = ezomero.post_file_annotation(\n",
    "            conn,\n",
    "            str(label_file),\n",
    "            ns='microsam.labelimage',\n",
    "            object_type=\"Image\",\n",
    "            object_id=image.getId(),\n",
    "            description=f'SAM {\"volumetric\" if is_volumetric else \"manual\"} segmentation ({model_type}){patch_desc}'\n",
    "        )\n",
    "        \n",
    "        if shapes:  # Only create ROI if shapes were found\n",
    "            roi_id = ezomero.post_roi(\n",
    "                conn,\n",
    "                image.getId(),\n",
    "                shapes,\n",
    "                name=f'SAM_{model_type}{\"_3D\" if is_volumetric else \"\"}{patch_desc}',\n",
    "                description=f'micro_sam.{\"volumetric\" if is_volumetric else \"manual\"}_instance_segmentation.{model_type}{patch_desc}'\n",
    "            )\n",
    "        else:\n",
    "            roi_id = None\n",
    "            \n",
    "        return label_id, roi_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_container(conn, datatype, container_id):\n",
    "    \"\"\"\n",
    "    Extract all images from a given OMERO container (Project, Dataset, Plate, Screen)\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        datatype: Type of container ('project', 'dataset', 'plate', 'screen', 'image')\n",
    "        container_id: ID of the container\n",
    "        \n",
    "    Returns:\n",
    "        list: List of OMERO image objects\n",
    "        str: Description of the source (for tracking)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    source_desc = \"\"\n",
    "    \n",
    "    if datatype == \"image\":\n",
    "        image = conn.getObject(\"Image\", container_id)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image with ID {container_id} not found\")\n",
    "        images = [image]\n",
    "        source_desc = f\"Image: {image.getName()} (ID: {container_id})\"\n",
    "    \n",
    "    elif datatype == \"dataset\":\n",
    "        dataset = conn.getObject(\"Dataset\", container_id)\n",
    "        if dataset is None:\n",
    "            raise ValueError(f\"Dataset with ID {container_id} not found\")\n",
    "        images = list(dataset.listChildren())\n",
    "        source_desc = f\"Dataset: {dataset.getName()} (ID: {container_id})\"\n",
    "    \n",
    "    elif datatype == \"project\":\n",
    "        project = conn.getObject(\"Project\", container_id)\n",
    "        if project is None:\n",
    "            raise ValueError(f\"Project with ID {container_id} not found\")\n",
    "        # Get all datasets in the project\n",
    "        for dataset in project.listChildren():\n",
    "            # Get all images in each dataset\n",
    "            for image in dataset.listChildren():\n",
    "                images.append(image)\n",
    "        source_desc = f\"Project: {project.getName()} (ID: {container_id})\"\n",
    "    \n",
    "    elif datatype == \"plate\":\n",
    "        plate = conn.getObject(\"Plate\", container_id)\n",
    "        if plate is None:\n",
    "            raise ValueError(f\"Plate with ID {container_id} not found\")\n",
    "        # Get all wells in the plate\n",
    "        for well in plate.listChildren():\n",
    "            # Get all images (fields) in each well\n",
    "            for wellSample in well.listChildren():\n",
    "                images.append(wellSample.getImage())\n",
    "        source_desc = f\"Plate: {plate.getName()} (ID: {container_id})\"\n",
    "    \n",
    "    elif datatype == \"screen\":\n",
    "        screen = conn.getObject(\"Screen\", container_id)\n",
    "        if screen is None:\n",
    "            raise ValueError(f\"Screen with ID {container_id} not found\")\n",
    "        # Get all plates in the screen\n",
    "        for plate in screen.listChildren():\n",
    "            # Get all wells in each plate\n",
    "            for well in plate.listChildren():\n",
    "                # Get all images (fields) in each well\n",
    "                for wellSample in well.listChildren():\n",
    "                    images.append(wellSample.getImage())\n",
    "        source_desc = f\"Screen: {screen.getName()} (ID: {container_id})\"\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported datatype: {datatype}\")\n",
    "    \n",
    "    print(f\"Found {len(images)} images from {source_desc}\")\n",
    "    return images, source_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### New patch generation functions for extracting image regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch_coordinates(image_width, image_height, patch_size, num_patches, random_patches=True):\n",
    "    \"\"\"\n",
    "    Generate coordinates for image patches\n",
    "    \n",
    "    Args:\n",
    "        image_width: Width of the full image\n",
    "        image_height: Height of the full image\n",
    "        patch_size: Tuple of (width, height) for the patch\n",
    "        num_patches: Number of patches to generate\n",
    "        random_patches: If True, generate random patches; if False, generate centered patches\n",
    "        \n",
    "    Returns:\n",
    "        list: List of patch coordinates as tuples (x, y, width, height)\n",
    "    \"\"\"\n",
    "    patch_width, patch_height = patch_size\n",
    "    \n",
    "    # Ensure patch size is not larger than image\n",
    "    patch_width = min(patch_width, image_width)\n",
    "    patch_height = min(patch_height, image_height)\n",
    "    \n",
    "    patches = []\n",
    "    \n",
    "    if random_patches:\n",
    "        # Generate random patches\n",
    "        for _ in range(num_patches):\n",
    "            # Calculate valid coordinate ranges\n",
    "            max_x = image_width - patch_width\n",
    "            max_y = image_height - patch_height\n",
    "            \n",
    "            if max_x <= 0 or max_y <= 0:\n",
    "                # Image is too small for the patch, use full image\n",
    "                patches.append((0, 0, image_width, image_height))\n",
    "            else:\n",
    "                # Generate random coordinates\n",
    "                x = np.random.randint(0, max_x + 1)\n",
    "                y = np.random.randint(0, max_y + 1)\n",
    "                patches.append((x, y, patch_width, patch_height))\n",
    "    else:\n",
    "        # Generate centered patch\n",
    "        x = (image_width - patch_width) // 2\n",
    "        y = (image_height - patch_height) // 2\n",
    "        \n",
    "        # Add the centered patch (potentially multiple times if num_patches > 1)\n",
    "        for _ in range(num_patches):\n",
    "            patches.append((x, y, patch_width, patch_height))\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def extract_patch(image_array, patch_coords):\n",
    "    \"\"\"\n",
    "    Extract a patch from an image array\n",
    "    \n",
    "    Args:\n",
    "        image_array: Numpy array containing the image data\n",
    "        patch_coords: Tuple of (x, y, width, height)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Extracted patch\n",
    "    \"\"\"\n",
    "    x, y, width, height = patch_coords\n",
    "    \n",
    "    # Handle different dimensionality\n",
    "    if image_array.ndim == 2:\n",
    "        # 2D image\n",
    "        return image_array[y:y+height, x:x+width]\n",
    "    elif image_array.ndim == 3:\n",
    "        # 3D image (z-stack or multi-channel)\n",
    "        return image_array[:, y:y+height, x:x+width]\n",
    "    else:\n",
    "        # Higher dimensions (e.g., z-stack + multi-channel)\n",
    "        return image_array[..., y:y+height, x:x+width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Dask Lazy Loading Functions for OMERO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dask_image(conn, image_id, z_slice=None, timepoint=None, channel=None, three_d=False, patch_coords=None):\n",
    "    \"\"\"\n",
    "    Get a dask array representation of an OMERO image for lazy loading\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        image_id: ID of image to load\n",
    "        z_slice: Optional specific Z slice to load (int or list)\n",
    "        timepoint: Optional specific timepoint to load (int or list)\n",
    "        channel: Optional specific channel to load (int or list)\n",
    "        three_d: Whether to load a 3D volume (all z-slices) instead of a single slice\n",
    "        patch_coords: Optional tuple of (x, y, width, height) to extract a patch\n",
    "    \n",
    "    Returns:\n",
    "        dask array representation of image\n",
    "    \"\"\"\n",
    "    image = conn.getObject(\"Image\", image_id)\n",
    "    pixels = image.getPrimaryPixels()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    size_z = image.getSizeZ()\n",
    "    size_c = image.getSizeC()\n",
    "    size_t = image.getSizeT()\n",
    "    size_y = image.getSizeY()\n",
    "    size_x = image.getSizeX()\n",
    "    \n",
    "    # Define specific dimensions to load if provided\n",
    "    # If three_d is True, we want all z-slices, otherwise use the provided z_slice\n",
    "    if three_d:\n",
    "        z_range = range(size_z)  # Load all z-slices for 3D\n",
    "    else:\n",
    "        z_range = [z_slice] if isinstance(z_slice, int) else (range(size_z) if z_slice is None else z_slice)\n",
    "    \n",
    "    t_range = [timepoint] if isinstance(timepoint, int) else (range(size_t) if timepoint is None else timepoint)\n",
    "    c_range = [channel] if isinstance(channel, int) else (range(size_c) if channel is None else channel)\n",
    "    \n",
    "    # Extract patch information if provided\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "    if patch_coords:\n",
    "        x_offset, y_offset, patch_width, patch_height = patch_coords\n",
    "        size_x = patch_width\n",
    "        size_y = patch_height\n",
    "    \n",
    "    # Create empty dict to store delayed objects\n",
    "    delayed_planes = {}\n",
    "    \n",
    "    desc = \"patch\" if patch_coords else \"image\"\n",
    "    print(f\"Creating dask array for {desc} {image_id} with lazy loading\")\n",
    "    print(f\"Dimensions: Z={len(z_range)}, C={len(c_range)}, T={len(t_range)}, Y={size_y}, X={size_x}\")\n",
    "    print(f\"3D mode: {three_d}\")\n",
    "    \n",
    "    # Create lazy loading function\n",
    "    @dask.delayed\n",
    "    def get_plane(z, c, t):\n",
    "        print(f\"Loading plane: Z={z}, C={c}, T={t}\")\n",
    "        if patch_coords:\n",
    "            full_plane = pixels.getPlane(z, c, t)\n",
    "            return full_plane[y_offset:y_offset+size_y, x_offset:x_offset+size_x]\n",
    "        else:\n",
    "            return pixels.getPlane(z, c, t)\n",
    "    \n",
    "    # Build dask arrays\n",
    "    arrays = []\n",
    "    for t in t_range:\n",
    "        t_arrays = []\n",
    "        for z in z_range:\n",
    "            z_arrays = []\n",
    "            for c in c_range:\n",
    "                # Create a key for this plane\n",
    "                key = (z, c, t)\n",
    "                \n",
    "                # Check if we've already created this delayed object\n",
    "                if key not in delayed_planes:\n",
    "                    # Create a delayed object for this plane\n",
    "                    delayed_plane = get_plane(z, c, t)\n",
    "                    delayed_planes[key] = delayed_plane\n",
    "                else:\n",
    "                    delayed_plane = delayed_planes[key]\n",
    "                \n",
    "                # Convert to dask array with known shape and dtype\n",
    "                shape = (size_y, size_x)\n",
    "                dtype = np.uint16  # Most OMERO images use 16-bit\n",
    "                dask_plane = da.from_delayed(delayed_plane, shape=shape, dtype=dtype)\n",
    "                z_arrays.append(dask_plane)\n",
    "            if z_arrays:\n",
    "                # Stack channels for this z position\n",
    "                t_arrays.append(da.stack(z_arrays))\n",
    "        if t_arrays:\n",
    "            # Stack z-planes for this timepoint\n",
    "            arrays.append(da.stack(t_arrays))\n",
    "    \n",
    "    if arrays:\n",
    "        # Stack all timepoints\n",
    "        return da.stack(arrays)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def store_annotations_in_zarr(mask_data, output_folder, image_num):\n",
    "    \"\"\"\n",
    "    Store annotation masks in zarr format for efficient access\n",
    "    \n",
    "    Args:\n",
    "        mask_data: Numpy array with mask data\n",
    "        output_folder: Base folder to store zarr data\n",
    "        image_num: Image number/identifier\n",
    "        \n",
    "    Returns:\n",
    "        path: Path to the zarr store\n",
    "    \"\"\"\n",
    "    # Create zarr directory if it doesn't exist\n",
    "    zarr_dir = os.path.join(output_folder, \"annotations\")\n",
    "    os.makedirs(zarr_dir, exist_ok=True)\n",
    "    \n",
    "    # Create zarr filename\n",
    "    zarr_path = os.path.join(zarr_dir, f\"annotation_{image_num:05d}.zarr\")\n",
    "    \n",
    "    # Remove existing zarr store if it exists\n",
    "    if os.path.exists(zarr_path):\n",
    "        shutil.rmtree(zarr_path)\n",
    "        \n",
    "    # Create zarr array from mask data\n",
    "    z = zarr.open(zarr_path, mode='w')\n",
    "    z.create_dataset('masks', data=mask_data, chunks=(256, 256))\n",
    "    \n",
    "    # Return path to zarr store\n",
    "    return zarr_path\n",
    "\n",
    "def zarr_to_tiff(zarr_path, output_tiff_path):\n",
    "    \"\"\"\n",
    "    Convert zarr store to TIFF file for OMERO upload\n",
    "    \n",
    "    Args:\n",
    "        zarr_path: Path to zarr store\n",
    "        output_tiff_path: Path to save TIFF file\n",
    "        \n",
    "    Returns:\n",
    "        output_tiff_path: Path to saved TIFF file\n",
    "    \"\"\"\n",
    "    # Load data from zarr\n",
    "    z = zarr.open(zarr_path, mode='r')\n",
    "    mask_data = z['masks'][:]\n",
    "    \n",
    "    # Save as TIFF\n",
    "    imwrite(output_tiff_path, mask_data)\n",
    "    \n",
    "    return output_tiff_path\n",
    "\n",
    "def cleanup_local_embeddings(output_folder):\n",
    "    \"\"\"\n",
    "    Check for and clean up any existing embeddings from previous interrupted runs\n",
    "    \n",
    "    Args:\n",
    "        output_folder: Path to the output folder containing embeddings\n",
    "    \"\"\"\n",
    "    embed_path = os.path.join(output_folder, \"embed\")\n",
    "    \n",
    "    if os.path.exists(embed_path):\n",
    "        # Look for embedding zarr directories and zip files\n",
    "        for item in os.listdir(embed_path):\n",
    "            item_path = os.path.join(embed_path, item)\n",
    "            if os.path.isdir(item_path) and \"embedding_\" in item and item.endswith(\".zarr\"):\n",
    "                print(f\"Cleaning up leftover embedding directory: {item}\")\n",
    "                shutil.rmtree(item_path)\n",
    "            elif os.path.isfile(item_path) and \"embedding_\" in item and item.endswith(\".zip\"):\n",
    "                print(f\"Cleaning up leftover embedding zip: {item}\")\n",
    "                os.remove(item_path)\n",
    "    \n",
    "    # Check output directory for segmentation files\n",
    "    output_path = os.path.join(output_folder, \"output\")\n",
    "    if os.path.exists(output_path):\n",
    "        for item in os.listdir(output_path):\n",
    "            item_path = os.path.join(output_path, item)\n",
    "            if os.path.isfile(item_path) and \"seg_\" in item and (item.endswith(\".tif\") or item.endswith(\".tiff\")):\n",
    "                print(f\"Cleaning up leftover segmentation file: {item}\")\n",
    "                os.remove(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_omero_batch_with_dask(\n",
    "    images_list,\n",
    "    output_folder: str,\n",
    "    container_type: str,\n",
    "    container_id: int,\n",
    "    source_desc: str,\n",
    "    model_type: str = 'vit_l',\n",
    "    batch_size: int = 3,\n",
    "    channel: int = 0,\n",
    "    timepoints: list = [0],\n",
    "    timepoint_mode: str = \"specific\",\n",
    "    z_slices: list = [0],\n",
    "    z_slice_mode: str = \"specific\",\n",
    "    segment_all: bool = True,\n",
    "    train_n: int = 3,\n",
    "    validate_n: int = 3,\n",
    "    three_d: bool = False,\n",
    "    use_patches: bool = False,\n",
    "    patch_size: tuple = (512, 512),\n",
    "    patches_per_image: int = 1,\n",
    "    random_patches: bool = True,\n",
    "    resume_from_table: bool = False,\n",
    "    read_only_mode: bool = False,\n",
    "    local_output_dir: str = \"./omero_annotations\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process OMERO images in batches for SAM segmentation using dask for lazy loading\n",
    "    and zarr for temporary annotation storage\n",
    "    \n",
    "    Args:\n",
    "        images_list: List of OMERO image objects\n",
    "        output_folder: Path to store temporary files\n",
    "        container_type: Type of OMERO container ('dataset', 'plate', 'project', 'screen', 'image')\n",
    "        container_id: ID of the container\n",
    "        source_desc: Description of the container (for tracking)\n",
    "        model_type: SAM model type\n",
    "        batch_size: Number of images/patches to process at once\n",
    "        channel: Channel to segment\n",
    "        timepoints: List of timepoints to process\n",
    "        timepoint_mode: How to handle timepoints (\"all\", \"random\", \"specific\")\n",
    "        z_slices: List of Z-slices to process (used only when three_d=False)\n",
    "        z_slice_mode: How to handle z-slices (\"all\", \"random\", \"specific\")\n",
    "        segment_all: Segment all images in the dataset or only train/validate subset\n",
    "        train_n: Number of training images if not segment_all\n",
    "        validate_n: Number of validation images if not segment_all\n",
    "        three_d: Whether to use 3D volumetric mode\n",
    "        use_patches: Whether to extract and process patches instead of full images\n",
    "        patch_size: Size of patches to extract (width, height)\n",
    "        patches_per_image: Number of patches to extract from each image (if random_patches=True)\n",
    "        random_patches: Whether to extract random patches or centered patches\n",
    "        resume_from_table: Whether to resume annotation from an existing tracking table\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (table_id, combined_images)\n",
    "    \"\"\"\n",
    "    # Setup output directories\n",
    "    output_path = os.path.join(output_folder, \"output\")\n",
    "    embed_path = os.path.join(output_folder, \"embed\")\n",
    "    zarr_path = os.path.join(output_folder, \"zarr\")\n",
    "    \n",
    "    # Check for and clean up any existing embeddings from interrupted runs\n",
    "    cleanup_local_embeddings(output_folder)\n",
    "    \n",
    "    # Remove directories if they exist\n",
    "    for path in [output_path, embed_path, zarr_path]:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # Create or retrieve tracking DataFrame with additional columns for the new features\n",
    "    df = pd.DataFrame(columns=[\n",
    "        \"image_id\", \"image_name\", \"train\", \"validate\", \n",
    "        \"channel\", \"z_slice\", \"timepoint\", \"sam_model\", \"embed_id\", \"label_id\", \"roi_id\", \n",
    "        \"is_volumetric\", \"processed\", \"is_patch\", \"patch_x\", \"patch_y\", \"patch_width\", \"patch_height\",\n",
    "        \"schema_attachment_id\"  # New column for schema attachment\n",
    "    ])\n",
    "    \n",
    "    table_id = None\n",
    "    \n",
    "    # Check if we should resume from an existing table\n",
    "    if resume_from_table:\n",
    "        try:\n",
    "            # Get existing tracking table\n",
    "            existing_tables = ezomero.get_table_names(conn, container_type.capitalize(), container_id)\n",
    "            if \"micro_sam_training_data\" in existing_tables:\n",
    "                # Get the table ID and data\n",
    "                table_ids = ezomero.get_table_ids(conn, container_type.capitalize(), container_id)\n",
    "                for tid in table_ids:\n",
    "                    table_name = ezomero.get_table_names(conn, container_type.capitalize(), container_id, tid)\n",
    "                    if table_name == \"micro_sam_training_data\":\n",
    "                        table_id = tid\n",
    "                        existing_df = ezomero.get_table(conn, table_id)\n",
    "                        \n",
    "                        # Add any missing columns (for backward compatibility)\n",
    "                        for col in df.columns:\n",
    "                            if col not in existing_df.columns:\n",
    "                                existing_df[col] = None\n",
    "                        \n",
    "                        # Ensure schema_attachment_id column exists if resuming\n",
    "                        if 'schema_attachment_id' not in existing_df.columns:\n",
    "                            existing_df['schema_attachment_id'] = None\n",
    "                                \n",
    "                        df = existing_df\n",
    "                        \n",
    "                        print(f\"Resuming from existing table ID: {table_id}\")\n",
    "                        print(f\"Found {len(df)} previously processed images/patches\")\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving existing table: {e}. Starting fresh.\")\n",
    "            resume_from_table = False\n",
    "    \n",
    "    # Get images list (already provided as argument)\n",
    "    combined_images_sequence = np.zeros(len(images_list))  # Initialize sequence array\n",
    "    \n",
    "    # Select images based on segment_all flag\n",
    "    if segment_all:\n",
    "        combined_images = images_list\n",
    "        combined_images_sequence = np.zeros(len(combined_images))  # All treated as training\n",
    "    else:\n",
    "        # Check if we have enough images\n",
    "        if len(images_list) < train_n + validate_n:\n",
    "            print(\"Not enough images in container for training and validation\")\n",
    "            raise ValueError(f\"Need at least {train_n + validate_n} images but found {len(images_list)}\")\n",
    "            \n",
    "        # Select random images for training and validation\n",
    "        train_indices = np.random.choice(len(images_list), train_n, replace=False)\n",
    "        train_images = [images_list[i] for i in train_indices]\n",
    "        \n",
    "        # Get validation images from the remaining ones\n",
    "        validate_candidates = [img for i, img in enumerate(images_list) if i not in train_indices]\n",
    "        validate_images = np.random.choice(validate_candidates, validate_n, replace=False)\n",
    "        \n",
    "        # Interleave the arrays and create sequence markers\n",
    "        combined_images, combined_images_sequence = interleave_arrays(train_images, validate_images)\n",
    "    \n",
    "    # If resuming, filter out already processed images/patches\n",
    "    processing_units = []  # Will contain tuples of (image, sequence_val, [metadata])\n",
    "    \n",
    "    if resume_from_table and len(df) > 0:\n",
    "        # For patch mode, we need to check image_id + patch coordinates\n",
    "        if use_patches:\n",
    "            # Get list of already processed image+patch combinations\n",
    "            processed_patches = set()\n",
    "            for _, row in df[df['processed'] == True].iterrows():\n",
    "                patch_key = (row['image_id'], row.get('patch_x', 0), row.get('patch_y', 0), \n",
    "                             row.get('patch_width', 0), row.get('patch_height', 0))\n",
    "                processed_patches.add(patch_key)\n",
    "            \n",
    "            # Generate all possible patches\n",
    "            for i, img in enumerate(combined_images):\n",
    "                img_id = img.getId()\n",
    "                seq_val = combined_images_sequence[i]\n",
    "                \n",
    "                # Get image dimensions\n",
    "                size_x = img.getSizeX()\n",
    "                size_y = img.getSizeY()\n",
    "                \n",
    "                # Generate patches for this image\n",
    "                img_patches = generate_patch_coordinates(\n",
    "                    size_x, size_y, patch_size, patches_per_image, random_patches)\n",
    "                \n",
    "                # Filter out already processed patches\n",
    "                for patch in img_patches:\n",
    "                    patch_key = (img_id, patch[0], patch[1], patch[2], patch[3])\n",
    "                    if patch_key not in processed_patches:\n",
    "                        processing_units.append((img, seq_val, patch))\n",
    "                        \n",
    "            print(f\"Found {len(processing_units)} remaining patches to process\")\n",
    "            \n",
    "        else:\n",
    "            # Get list of already processed image IDs\n",
    "            processed_ids = set(df[df['processed'] == True]['image_id'].values)\n",
    "            \n",
    "            # Filter combined_images\n",
    "            for i, img in enumerate(combined_images):\n",
    "                if img.getId() not in processed_ids:\n",
    "                    processing_units.append((img, combined_images_sequence[i], None))\n",
    "            \n",
    "            print(f\"Found {len(processing_units)} remaining images to process\")\n",
    "    else:\n",
    "        # Not resuming, generate all processing units\n",
    "        if use_patches:\n",
    "            # Generate patches for all images\n",
    "            for i, img in enumerate(combined_images):\n",
    "                seq_val = combined_images_sequence[i]\n",
    "                \n",
    "                # Get image dimensions\n",
    "                size_x = img.getSizeX()\n",
    "                size_y = img.getSizeY()\n",
    "                \n",
    "                # Generate patches for this image\n",
    "                img_patches = generate_patch_coordinates(\n",
    "                    size_x, size_y, patch_size, patches_per_image, random_patches)\n",
    "                \n",
    "                for patch in img_patches:\n",
    "                    processing_units.append((img, seq_val, patch))\n",
    "                    \n",
    "            print(f\"Generated {len(processing_units)} patches to process\")\n",
    "        else:\n",
    "            # Use full images\n",
    "            for i, img in enumerate(combined_images):\n",
    "                processing_units.append((img, combined_images_sequence[i], None))\n",
    "    \n",
    "    # Calculate total number of batches\n",
    "    total_batches = (len(processing_units) + batch_size - 1) // batch_size\n",
    "    \n",
    "    if use_patches:\n",
    "        print(f\"Processing {len(processing_units)} patches in {total_batches} batches\")\n",
    "    else:\n",
    "        print(f\"Processing {len(processing_units)} images in {total_batches} batches\")\n",
    "    \n",
    "    print(f\"3D mode: {three_d}\")\n",
    "    \n",
    "    # Process images/patches in batches\n",
    "    for batch_idx in range(total_batches):\n",
    "        print(f\"\\nProcessing batch {batch_idx+1}/{total_batches}\")\n",
    "        \n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(processing_units))\n",
    "        batch_units = processing_units[start_idx:end_idx]\n",
    "        \n",
    "        # Load batch images as dask arrays for lazy loading\n",
    "        images = []\n",
    "        dask_images = []\n",
    "        image_data = []  # Store metadata about each image/patch\n",
    "        \n",
    "        for unit_idx, (image, seq_val, patch) in enumerate(batch_units):\n",
    "            image_id = image.getId()\n",
    "            \n",
    "            # Determine which timepoint to use\n",
    "            if timepoint_mode == \"all\":\n",
    "                # Use all timepoints (not yet supported in this function)\n",
    "                actual_timepoint = timepoints[0]  # Default to first timepoint for now\n",
    "                print(\"Warning: 'all' timepoint mode not fully supported yet, using first timepoint\")\n",
    "            elif timepoint_mode == \"random\":\n",
    "                # Select a random timepoint from the list\n",
    "                actual_timepoint = np.random.choice(timepoints)\n",
    "            else:  # \"specific\"\n",
    "                # Use the first timepoint in the list\n",
    "                actual_timepoint = timepoints[0]\n",
    "            \n",
    "            # For 3D mode or 2D with patches\n",
    "            if three_d:\n",
    "                # 3D mode - process entire Z-stack or specified z-range\n",
    "                pixels = image.getPrimaryPixels()\n",
    "                \n",
    "                if patch is not None:\n",
    "                    # Extract 3D patch (x, y, z-stack)\n",
    "                    x, y, width, height = patch\n",
    "                    img_3d = np.zeros((image.getSizeZ(), height, width), dtype=np.uint16)\n",
    "                    \n",
    "                    # Load each z-slice for the patch\n",
    "                    for z in range(image.getSizeZ()):\n",
    "                        full_plane = pixels.getPlane(z, channel, actual_timepoint)\n",
    "                        img_3d[z] = full_plane[y:y+height, x:x+width]\n",
    "                        \n",
    "                    # Record metadata\n",
    "                    image_data.append({\n",
    "                        'image_id': image_id, \n",
    "                        'sequence': seq_val,\n",
    "                        'timepoint': actual_timepoint,\n",
    "                        'z_slice': 'all',\n",
    "                        'is_patch': True,\n",
    "                        'patch_x': x,\n",
    "                        'patch_y': y,\n",
    "                        'patch_width': width,\n",
    "                        'patch_height': height\n",
    "                    })\n",
    "                else:\n",
    "                    # Process full 3D volume\n",
    "                    img_3d = np.stack([pixels.getPlane(z, channel, actual_timepoint) \n",
    "                                     for z in range(image.getSizeZ())])\n",
    "                    \n",
    "                    # Record metadata\n",
    "                    image_data.append({\n",
    "                        'image_id': image_id, \n",
    "                        'sequence': seq_val,\n",
    "                        'timepoint': actual_timepoint,\n",
    "                        'z_slice': 'all',\n",
    "                        'is_patch': False,\n",
    "                        'patch_x': 0,\n",
    "                        'patch_y': 0,\n",
    "                        'patch_width': image.getSizeX(),\n",
    "                        'patch_height': image.getSizeY()\n",
    "                    })\n",
    "                \n",
    "                images.append(img_3d)\n",
    "                print(f\"Loaded 3D image/patch for image {image_id} with shape {img_3d.shape}\")\n",
    "                \n",
    "            else:\n",
    "                # 2D mode - determine which z-slice to use\n",
    "                if z_slice_mode == \"all\":\n",
    "                    # Use all z-slices (not yet supported in this function)\n",
    "                    actual_z_slice = z_slices[0]  # Default to first z-slice for now\n",
    "                    print(\"Warning: 'all' z-slice mode not fully supported yet, using first z-slice\")\n",
    "                elif z_slice_mode == \"random\":\n",
    "                    # Select a random z-slice from the list\n",
    "                    actual_z_slice = np.random.choice(z_slices)\n",
    "                else:  # \"specific\"\n",
    "                    # Use the first z-slice in the list\n",
    "                    actual_z_slice = z_slices[0]\n",
    "                \n",
    "                pixels = image.getPrimaryPixels()\n",
    "                \n",
    "                if patch is not None:\n",
    "                    # Extract 2D patch from the specified plane\n",
    "                    x, y, width, height = patch\n",
    "                    full_plane = pixels.getPlane(actual_z_slice, channel, actual_timepoint)\n",
    "                    img = full_plane[y:y+height, x:x+width]\n",
    "                    \n",
    "                    # Record metadata\n",
    "                    image_data.append({\n",
    "                        'image_id': image_id, \n",
    "                        'sequence': seq_val,\n",
    "                        'timepoint': actual_timepoint,\n",
    "                        'z_slice': actual_z_slice,\n",
    "                        'is_patch': True,\n",
    "                        'patch_x': x,\n",
    "                        'patch_y': y,\n",
    "                        'patch_width': width,\n",
    "                        'patch_height': height\n",
    "                    })\n",
    "                else:\n",
    "                    # Get full 2D plane\n",
    "                    img = pixels.getPlane(actual_z_slice, channel, actual_timepoint)\n",
    "                    \n",
    "                    # Record metadata\n",
    "                    image_data.append({\n",
    "                        'image_id': image_id, \n",
    "                        'sequence': seq_val,\n",
    "                        'timepoint': actual_timepoint,\n",
    "                        'z_slice': actual_z_slice,\n",
    "                        'is_patch': False,\n",
    "                        'patch_x': 0,\n",
    "                        'patch_y': 0,\n",
    "                        'patch_width': image.getSizeX(),\n",
    "                        'patch_height': image.getSizeY()\n",
    "                    })\n",
    "                \n",
    "                images.append(img)\n",
    "                print(f\"Loaded 2D image/patch for image {image_id} with shape {img.shape}\")\n",
    "        \n",
    "        # Process batch with SAM using standard numpy arrays\n",
    "        print(\"Starting napari viewer with SAM annotator. Close the viewer window when done.\")\n",
    "        \n",
    "        # Create viewer without context management\n",
    "        viewer = napari.Viewer()\n",
    "        \n",
    "        # Add image series annotator\n",
    "        image_series_annotator(\n",
    "            images, \n",
    "            model_type=model_type,\n",
    "            viewer=viewer,\n",
    "            embedding_path=os.path.join(output_folder, \"embed\"),\n",
    "            output_folder=os.path.join(output_folder, \"output\"),\n",
    "            is_volumetric=three_d\n",
    "        )\n",
    "        \n",
    "        # Start the napari application - this blocks until the viewer is closed\n",
    "        try:\n",
    "            napari.run()\n",
    "            print(\"Napari viewer closed.\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Napari viewer was interrupted. Processing results anyway...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in napari: {e}\")\n",
    "            \n",
    "        print(\"Processing results from batch...\")\n",
    "        print(\"Done annotating batch, storing results in zarr and uploading to OMERO\")\n",
    "        \n",
    "        # Initialize batch progress tracking\n",
    "        batch_completed = 0\n",
    "        batch_skipped = 0\n",
    "        \n",
    "        # Process results for batch\n",
    "        batch_df = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        for n, unit_data in enumerate(image_data):\n",
    "            local_n = n  # Index within current batch\n",
    "            global_n = start_idx + n  # Global index across all batches\n",
    "            \n",
    "            # Get the image object\n",
    "            image = conn.getObject(\"Image\", unit_data['image_id'])\n",
    "            is_patch = unit_data['is_patch']\n",
    "            patch_info = None\n",
    "            \n",
    "            if is_patch:\n",
    "                patch_info = (unit_data['patch_x'], unit_data['patch_y'], \n",
    "                             unit_data['patch_width'], unit_data['patch_height'])\n",
    "            \n",
    "            # Store segmentation mask in zarr before uploading to OMERO\n",
    "            seg_file_path = os.path.join(output_folder, \"output\", f\"seg_{local_n:05d}.tif\")\n",
    "            if not os.path.exists(seg_file_path):\n",
    "                print(f\"Warning: Segmentation file not found for image {image.getId()}, skipping\")\n",
    "                batch_skipped += 1\n",
    "                \n",
    "                # Add a row for skipped image but mark as not processed\n",
    "                is_train = unit_data['sequence'] == 0 if not segment_all else True\n",
    "                is_validate = unit_data['sequence'] == 1 if not segment_all else False\n",
    "                \n",
    "                # Z-slice information\n",
    "                z_info = 'all' if three_d else unit_data['z_slice']\n",
    "                \n",
    "                new_row = pd.DataFrame([{\n",
    "                    \"image_id\": image.getId(),\n",
    "                    \"image_name\": image.getName(),\n",
    "                    \"train\": is_train,\n",
    "                    \"validate\": is_validate,\n",
    "                    \"channel\": channel,\n",
    "                    \"z_slice\": z_info,\n",
    "                    \"timepoint\": unit_data['timepoint'],\n",
    "                    \"sam_model\": model_type,\n",
    "                    \"embed_id\": None,\n",
    "                    \"label_id\": None,\n",
    "                    \"roi_id\": None,\n",
    "                    \"is_volumetric\": three_d,\n",
    "                    \"processed\": False,\n",
    "                    \"is_patch\": is_patch,\n",
    "                    \"patch_x\": unit_data.get('patch_x', 0),\n",
    "                    \"patch_y\": unit_data.get('patch_y', 0),\n",
    "                    \"patch_width\": unit_data.get('patch_width', 0),\n",
    "                    \"patch_height\": unit_data.get('patch_height', 0)\n",
    "                }])\n",
    "                batch_df = pd.concat([batch_df, new_row], ignore_index=True)\n",
    "                continue\n",
    "                \n",
    "            batch_completed += 1\n",
    "            \n",
    "            # Read the segmentation mask\n",
    "            mask_data = imageio.imread(seg_file_path)\n",
    "            \n",
    "            # Store in zarr format for efficient processing\n",
    "            zarr_file_path = store_annotations_in_zarr(mask_data, zarr_path, global_n)\n",
    "            \n",
    "            # Store embedding in zarr format and zip for OMERO upload\n",
    "            embed_zarr = f\"embedding_{local_n:05d}.zarr\"\n",
    "            embed_dir = os.path.join(output_folder, \"embed\")\n",
    "            zip_path = os.path.join(embed_dir, f\"embedding_{global_n:05d}.zip\")\n",
    "            \n",
    "            # Check if the embedding directory exists before trying to zip it\n",
    "            embed_zarr_path = os.path.join(embed_dir, embed_zarr)\n",
    "            if not os.path.exists(embed_zarr_path):\n",
    "                print(f\"Warning: Embedding directory {embed_zarr} not found, skipping embedding upload\")\n",
    "                embed_id = None\n",
    "            else:\n",
    "                with zipfile.ZipFile(zip_path, 'w') as zip_file:\n",
    "                    zip_directory(embed_dir, embed_zarr, zip_file)\n",
    "                \n",
    "                # Upload embedding to OMERO\n",
    "                embed_id = ezomero.post_file_annotation(\n",
    "                    conn,\n",
    "                    str(zip_path),\n",
    "                    ns='microsam.embeddings',\n",
    "                    object_type=\"Image\",\n",
    "                    object_id=image.getId(),\n",
    "                    description=f'SAM embedding ({model_type}), 3D={three_d}, Patch={is_patch}'\n",
    "                )\n",
    "            \n",
    "            # Convert zarr annotation to TIFF for OMERO compatibility\n",
    "            tiff_path = os.path.join(output_folder, \"output\", f\"seg_{global_n:05d}.tiff\")\n",
    "            zarr_to_tiff(zarr_file_path, tiff_path)\n",
    "            \n",
    "            # For ROI creation, we need to handle patches differently\n",
    "            if is_patch:\n",
    "                # We need to create ROIs with the proper offset in the original image\n",
    "                patch_x, patch_y = patch_info\n",
    "            else:\n",
    "                patch_x, patch_y = 0, 0\n",
    "                \n",
    "            # Upload labels and create ROIs - handle 3D and patches\n",
    "            if three_d:\n",
    "                # For 3D data, handle z-dimension correctly\n",
    "                z_for_roi = range(image.getSizeZ())\n",
    "                label_id, roi_id = upload_rois_and_labels(\n",
    "                    conn, \n",
    "                    image, \n",
    "                    tiff_path, \n",
    "                    z_for_roi,\n",
    "                    channel, \n",
    "                    unit_data['timepoint'], \n",
    "                    model_type,\n",
    "                    is_volumetric=True,\n",
    "                    patch_offset=(patch_x, patch_y) if is_patch else None,\n",
    "                    read_only_mode=read_only_mode,\n",
    "                    local_output_dir=local_output_dir\n",
    "                )\n",
    "            else:\n",
    "                # For 2D data - with potential patch offset\n",
    "                label_id, roi_id = upload_rois_and_labels(\n",
    "                    conn, \n",
    "                    image, \n",
    "                    tiff_path, \n",
    "                    unit_data['z_slice'], \n",
    "                    channel, \n",
    "                    unit_data['timepoint'], \n",
    "                    model_type,\n",
    "                    is_volumetric=False,\n",
    "                    patch_offset=(patch_x, patch_y) if is_patch else None,\n",
    "                    read_only_mode=read_only_mode,\n",
    "                    local_output_dir=local_output_dir\n",
    "                )\n",
    "            \n",
    "            # Update tracking dataframe\n",
    "            is_train = unit_data['sequence'] == 0 if not segment_all else True\n",
    "            is_validate = unit_data['sequence'] == 1 if not segment_all else False\n",
    "            \n",
    "            # Z-slice information\n",
    "            z_info = 'all' if three_d else unit_data['z_slice']\n",
    "            \n",
    "            new_row = pd.DataFrame([{\n",
    "                \"image_id\": image.getId(),\n",
    "                \"image_name\": image.getName(),\n",
    "                \"train\": is_train,\n",
    "                \"validate\": is_validate,\n",
    "                \"channel\": channel,\n",
    "                \"z_slice\": z_info,\n",
    "                \"timepoint\": unit_data['timepoint'],\n",
    "                \"sam_model\": model_type,\n",
    "                \"embed_id\": embed_id,\n",
    "                \"label_id\": label_id,\n",
    "                \"roi_id\": roi_id,\n",
    "                \"is_volumetric\": three_d,\n",
    "                \"processed\": True,\n",
    "                \"is_patch\": is_patch,\n",
    "                \"patch_x\": unit_data.get('patch_x', 0),\n",
    "                \"patch_y\": unit_data.get('patch_y', 0),\n",
    "                \"patch_width\": unit_data.get('patch_width', 0),\n",
    "                \"patch_height\": unit_data.get('patch_height', 0)\n",
    "            }])\n",
    "            batch_df = pd.concat([batch_df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Update the main DataFrame with the batch results\n",
    "        df = pd.concat([df, batch_df], ignore_index=True)\n",
    "        \n",
    "        # Upload batch tracking table to OMERO\n",
    "        if table_id is not None:\n",
    "            # Delete the existing table before creating a new one\n",
    "            try:\n",
    "                print(f\"Deleting existing table with ID: {table_id}\")\n",
    "                # Get the file annotation object for the table\n",
    "                ann = conn.getObject(\"FileAnnotation\", table_id)\n",
    "                if ann:\n",
    "                    # Delete the file annotation (which contains the table)\n",
    "                    conn.deleteObjects(\"FileAnnotation\", [table_id], wait=True)\n",
    "                    print(f\"Existing table deleted successfully\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find table with ID: {table_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete existing table: {e}\")\n",
    "                # Continue anyway, as we'll create a new table\n",
    "        \n",
    "        # Prepare DataFrame for OMERO table: Convert potentially None/NaN ID columns to string\n",
    "        df_for_omero = df.copy()\n",
    "        id_columns_to_convert = ['embed_id', 'label_id', 'roi_id', 'schema_attachment_id']\n",
    "        for col in id_columns_to_convert:\n",
    "            if col in df_for_omero.columns: # Ensure column exists\n",
    "                # Convert to string, handling potential float NaNs first if necessary\n",
    "                df_for_omero[col] = df_for_omero[col].astype(str)\n",
    "\n",
    "\n",
    "        # Create a new table with the updated data\n",
    "        table_id = ezomero.post_table(\n",
    "            conn, \n",
    "            object_type=container_type.capitalize(), \n",
    "            object_id=container_id, \n",
    "            table=df_for_omero, # Use the converted DataFrame\n",
    "            title=\"micro_sam_training_data\"\n",
    "        )\n",
    "        if table_id is None:\n",
    "            print(\"Warning: Failed to create tracking table\")\n",
    "        else:\n",
    "            print(f\"Created new tracking table with ID: {table_id}\")\n",
    "        \n",
    "        print(f\"Batch {batch_idx+1}/{total_batches} results:\")\n",
    "        print(f\"  - Completed: {batch_completed}/{len(batch_units)} units\")\n",
    "        print(f\"  - Skipped: {batch_skipped}/{len(batch_units)} units\")\n",
    "        \n",
    "        if batch_skipped > 0 and batch_idx < total_batches - 1:\n",
    "            # Ask user if they want to continue with next batch or stop here\n",
    "            try:\n",
    "                response = input(\"Some units were skipped. Continue with next batch? (y/n): \")\n",
    "                if response.lower() not in ['y', 'yes']:\n",
    "                    print(\"Stopping processing at user request.\")\n",
    "                    break\n",
    "            except:\n",
    "                # In case of non-interactive environment, continue by default\n",
    "                print(\"Non-interactive environment detected. Continuing with next batch.\")\n",
    "        \n",
    "        # Clean up temporary files for this batch\n",
    "        for n in range(batch_size):  # Use local indexing for cleanup\n",
    "            if start_idx + n >= len(processing_units):  # Skip if we've processed all units\n",
    "                continue\n",
    "                \n",
    "            embed_zip = os.path.join(output_folder, \"embed\", f\"embedding_{n:05d}.zip\")\n",
    "            embed_zarr = os.path.join(output_folder, \"embed\", f\"embedding_{n:05d}.zarr\")\n",
    "            seg_file = os.path.join(output_folder, \"output\", f\"seg_{n:05d}.tif\")\n",
    "            \n",
    "            for path in [embed_zip, seg_file]:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "                    \n",
    "            if os.path.exists(embed_zarr) and os.path.isdir(embed_zarr):\n",
    "                shutil.rmtree(embed_zarr)\n",
    "    \n",
    "    # Final statistics\n",
    "    total_processed = df[df['processed'] == True].shape[0]\n",
    "    total_skipped = df[df['processed'] == False].shape[0]\n",
    "    \n",
    "    print(f\"\\nAll batches completed.\")\n",
    "    print(f\"Total processed: {total_processed} units\")\n",
    "    print(f\"Total skipped: {total_skipped} units\")\n",
    "    print(f\"Final tracking table ID: {table_id} in {source_desc}\")\n",
    "    \n",
    "    return table_id, combined_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_local_outputs(local_dir, container_type, container_id, image_id, timepoint=0, z_slice=0, is_patch=False, patch_coords=None):\n",
    "    \"\"\"\n",
    "    Organize local storage for annotations when working with read-only OMERO servers\n",
    "    \n",
    "    Args:\n",
    "        local_dir: Base directory for local storage\n",
    "        container_type: Type of OMERO container ('dataset', 'plate', etc.)\n",
    "        container_id: ID of the container\n",
    "        image_id: ID of the image\n",
    "        timepoint: Time point index\n",
    "        z_slice: Z-slice index or 'all' for volumetric data\n",
    "        is_patch: Whether this is a patch of a larger image\n",
    "        patch_coords: Optional tuple of (x, y, width, height) for patch info\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with paths for various outputs\n",
    "    \"\"\"\n",
    "    # Create the base container directory\n",
    "    container_path = os.path.join(local_dir, f\"{container_type}_{container_id}\")\n",
    "    os.makedirs(container_path, exist_ok=True)\n",
    "    \n",
    "    # Create image-specific directory\n",
    "    image_path = os.path.join(container_path, f\"image_{image_id}\")\n",
    "    os.makedirs(image_path, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories for different outputs\n",
    "    embed_path = os.path.join(image_path, \"embeddings\")\n",
    "    label_path = os.path.join(image_path, \"labels\")\n",
    "    roi_path = os.path.join(image_path, \"rois\")\n",
    "    \n",
    "    for path in [embed_path, label_path, roi_path]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Determine file naming based on dimensionality and patch info\n",
    "    name_parts = []\n",
    "    \n",
    "    # Add z-slice info\n",
    "    if z_slice == 'all':\n",
    "        name_parts.append(\"vol\")  # Volumetric data\n",
    "    else:\n",
    "        name_parts.append(f\"z{z_slice}\")\n",
    "    \n",
    "    # Add timepoint info\n",
    "    name_parts.append(f\"t{timepoint}\")\n",
    "    \n",
    "    # Add patch info if applicable\n",
    "    if is_patch and patch_coords:\n",
    "        x, y, width, height = patch_coords\n",
    "        name_parts.append(f\"patch_x{x}_y{y}_w{width}_h{height}\")\n",
    "    \n",
    "    # Create base filename\n",
    "    base_name = \"_\".join(name_parts)\n",
    "    \n",
    "    # Return paths for different output types\n",
    "    return {\n",
    "        \"base_dir\": image_path,\n",
    "        \"embedding_dir\": embed_path,\n",
    "        \"embedding_path\": os.path.join(embed_path, f\"{base_name}_embedding.zip\"),\n",
    "        \"label_path\": os.path.join(label_path, f\"{base_name}_label.tiff\"),\n",
    "        \"roi_path\": os.path.join(roi_path, f\"{base_name}_rois.json\"),\n",
    "        \"metadata_path\": os.path.join(image_path, f\"{base_name}_metadata.json\"),\n",
    "        \"base_name\": base_name\n",
    "    }\n",
    "\n",
    "def save_annotations_schema(metadata_path, image_id, label_path, roi_data, annotation_metadata):\n",
    "    \"\"\"\n",
    "    Save annotation metadata and ROIs in a structured JSON schema for local storage\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to save the metadata JSON file\n",
    "        image_id: OMERO image ID\n",
    "        label_path: Path to the label image file\n",
    "        roi_data: List of ROI data extracted from label image\n",
    "        annotation_metadata: Dictionary with additional metadata\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        # Create the JSON schema\n",
    "        schema = {\n",
    "            \"schema_version\": \"1.0\",\n",
    "            \"created_at\": pd.Timestamp.now().isoformat(),\n",
    "            \"image\": {\n",
    "                \"id\": int(image_id),\n",
    "                \"name\": annotation_metadata.get(\"image_name\", \"\"),\n",
    "                \"server\": annotation_metadata.get(\"server\", \"\")\n",
    "            },\n",
    "            \"annotation\": {\n",
    "                \"model\": annotation_metadata.get(\"model_type\", \"\"),\n",
    "                \"is_volumetric\": annotation_metadata.get(\"is_volumetric\", False),\n",
    "                \"channel\": annotation_metadata.get(\"channel\", 0),\n",
    "                \"z_slice\": annotation_metadata.get(\"z_slice\", 0),\n",
    "                \"timepoint\": annotation_metadata.get(\"timepoint\", 0),\n",
    "                \"is_patch\": annotation_metadata.get(\"is_patch\", False),\n",
    "                \"patch_coords\": annotation_metadata.get(\"patch_coords\", None)\n",
    "            },\n",
    "            \"files\": {\n",
    "                \"label_path\": os.path.relpath(label_path, os.path.dirname(metadata_path)),\n",
    "                \"embedding_path\": annotation_metadata.get(\"embedding_path\", \"\")\n",
    "            },\n",
    "            \"rois\": roi_data\n",
    "        }\n",
    "        \n",
    "        # Write the schema to file\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(schema, f, indent=2)\n",
    "            \n",
    "        print(f\"Saved annotation schema to {metadata_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving annotation schema: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Load images from OMERO and open in napari with micro-sam annotator\n",
    "\n",
    "When using 3D mode (`three_d=True`), the notebook will process entire Z-stacks instead of single slices. This allows for volumetric annotation using micro-SAM's 3D capabilities.\n",
    "\n",
    "When using patch mode (`use_patches=True`), the notebook will extract smaller regions from large images for more efficient annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Running segmentation in batch\n",
    "\n",
    "Note: some warnings from napari are expected in the output here, generally not a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##input parameters\n",
    "model_type = 'vit_b_lm'\n",
    "segment_all = False\n",
    "train_n = 2   \n",
    "validate_n = 2\n",
    "channel = 3  # which channel to segment starting from 0\n",
    "\n",
    "# Z-slice handling (for 2D mode)\n",
    "z_slices = [4, 6, 8]  # List of z-slices to use (ignored if three_d=True)\n",
    "z_slice_mode = \"random\"  # Options: \"all\", \"random\", \"specific\" (uses z_slices[0])\n",
    "\n",
    "# Timepoint handling\n",
    "timepoints = [0]  # List of timepoints to use\n",
    "timepoint_mode = \"specific\"  # Options: \"all\", \"random\", \"specific\" (uses timepoints[0])\n",
    "\n",
    "# Patch extraction settings\n",
    "use_patches = False  # Set to True to extract and process image patches instead of full images\n",
    "patch_size = (512, 512)  # Size of patches to extract (width, height)\n",
    "patches_per_image = 2  # Number of patches to extract from each image (if random_patches=True)\n",
    "random_patches = True  # If True, extract random patches; if False, extract from image center\n",
    "\n",
    "# Batch processing settings\n",
    "batch_size = 2  # The number of images/patches to process at once in napari\n",
    "three_d = False  # Set to True for 3D volumetric processing\n",
    "resume_from_table = False  # Set to True to continue from a previous run\n",
    "\n",
    "# Read-only mode settings\n",
    "read_only_mode = False  # Set to True to save annotations locally instead of uploading to OMERO\n",
    "local_output_dir = \"./omero_annotations\"  # Directory where annotations will be saved when in read-only mode\n",
    "\n",
    "# Configure napari settings\n",
    "settings = get_settings()\n",
    "settings.application.ipy_interactive = False\n",
    "\n",
    "# Run batch processing with dask lazy loading\n",
    "# Get all images from the specified container\n",
    "images_list, source_desc = get_images_from_container(conn, datatype, data_id)\n",
    "\n",
    "if len(images_list) > 0:\n",
    "    table_id, processed_images = process_omero_batch_with_dask(\n",
    "        images_list=images_list,\n",
    "        output_folder=output_directory,\n",
    "        container_type=datatype,\n",
    "        container_id=data_id,\n",
    "        source_desc=source_desc,\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        channel=channel,\n",
    "        timepoints=timepoints,\n",
    "        timepoint_mode=timepoint_mode,\n",
    "        z_slices=z_slices,\n",
    "        z_slice_mode=z_slice_mode,\n",
    "        segment_all=segment_all,\n",
    "        train_n=train_n,\n",
    "        validate_n=validate_n,\n",
    "        three_d=three_d,\n",
    "        use_patches=use_patches,\n",
    "        patch_size=patch_size,\n",
    "        patches_per_image=patches_per_image,\n",
    "        random_patches=random_patches,\n",
    "        resume_from_table=resume_from_table,\n",
    "        read_only_mode=read_only_mode,\n",
    "        local_output_dir=local_output_dir\n",
    "    )\n",
    "    print(f\"Finished processing with dask lazy loading. Table ID: {table_id}\")\n",
    "    print(f\"To resume this session later, set resume_from_table=True\")\n",
    "else:\n",
    "    print(f\"No images found in the {datatype} with ID {data_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Resuming Annotation Sessions\n",
    "\n",
    "This notebook now supports resuming annotation sessions. If you need to stop annotating and continue later:\n",
    "\n",
    "1. Set `resume_from_table = True` in the parameters section\n",
    "2. Run the notebook as usual\n",
    "3. The system will automatically detect previously annotated images/patches and continue with the remaining ones\n",
    "\n",
    "This is useful for:\n",
    "- Long annotation sessions that need to be split over multiple days\n",
    "- Cases where napari was closed accidentally\n",
    "- Continuing after computer restarts or crashes\n",
    "\n",
    "The tracking table in OMERO keeps track of which images/patches have been successfully processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Examples for processing different scenarios\n",
    "\n",
    "Here are examples showing how to set up the notebook for different use cases:\n",
    "\n",
    "### Processing with multiple z-slices\n",
    "```python\n",
    "# Z-slice handling (for 2D mode)\n",
    "z_slices = [4, 8, 12, 16]  # List of multiple z-slices to use\n",
    "z_slice_mode = \"random\"  # Randomly select one z-slice from the list for each image\n",
    "three_d = False  # Use 2D mode\n",
    "```\n",
    "\n",
    "### Processing with time series\n",
    "```python\n",
    "# Timepoint handling\n",
    "timepoints = [0, 5, 10, 15]  # List of timepoints to process\n",
    "timepoint_mode = \"all\"  # Process all timepoints in the list\n",
    "```\n",
    "\n",
    "### Processing image patches\n",
    "```python\n",
    "# Patch extraction settings\n",
    "use_patches = True  # Enable patch extraction\n",
    "patch_size = (512, 512)  # Size of patches to extract\n",
    "patches_per_image = 3  # Extract multiple patches per image\n",
    "random_patches = True  # Extract patches from random locations\n",
    "```\n",
    "\n",
    "### Processing a 3D dataset\n",
    "```python\n",
    "# 3D settings\n",
    "three_d = True  # Enable 3D volumetric processing\n",
    "batch_size = 2  # Smaller batch size for 3D as it requires more memory\n",
    "```\n",
    "\n",
    "### Processing a large dataset in parts\n",
    "```python\n",
    "# For large datasets\n",
    "segment_all = False  # Don't process all images\n",
    "train_n = 10  # Process only 10 images for training\n",
    "validate_n = 5  # And 5 for validation\n",
    "resume_from_table = True  # Enable resuming from previous sessions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Using Read-Only Mode\n",
    "\n",
    "This notebook now supports read-only mode for working with OMERO servers where you have read-only access, such as the Image Data Resource (IDR).\n",
    "\n",
    "When working in read-only mode:\n",
    "1. All annotations (ROIs, segmentation masks, embeddings) are saved locally instead of being uploaded to OMERO\n",
    "2. Data is organized in a structured way on disk for easy access and further processing\n",
    "3. No write permissions are required on the OMERO server\n",
    "\n",
    "To use read-only mode:\n",
    "```python\n",
    "# Read-only mode settings\n",
    "read_only_mode = True  # Enable read-only mode\n",
    "local_output_dir = \"./omero_annotations\"  # Directory where annotations will be saved\n",
    "```\n",
    "\n",
    "The annotations will be organized in a directory structure as follows:\n",
    "```\n",
    "local_output_dir/\n",
    "   image_{image_id}/\n",
    "       roi_{filename}.json  # ROI metadata in JSON format\n",
    "       {filename}.tiff      # Segmentation mask\n",
    "       embeddings/          # SAM embeddings\n",
    "            embedding_{n}.zip\n",
    "   image_{another_image_id}/\n",
    "       ...\n",
    "```\n",
    "\n",
    "The JSON metadata files contain information about the ROIs, including:\n",
    "- Image ID and name\n",
    "- Model type used\n",
    "- Z-slice, channel, and timepoint information\n",
    "- Whether the annotation is volumetric\n",
    "- Patch offset (if using patch-based processing)\n",
    "\n",
    "### Working with a read-only OMERO server\n",
    "\n",
    "```python\n",
    "##input parameters\n",
    "model_type = 'vit_b'\n",
    "segment_all = False\n",
    "train_n = 2   \n",
    "validate_n = 2\n",
    "channel = 3  # which channel to segment\n",
    "\n",
    "# Z-slice handling\n",
    "z_slices = [4, 6, 8]  \n",
    "z_slice_mode = \"random\"  \n",
    "\n",
    "# Read-only mode settings\n",
    "read_only_mode = True  # Enable read-only mode\n",
    "local_output_dir = \"./omero_idr_annotations\"  # Save annotations locally\n",
    "\n",
    "# Other settings\n",
    "batch_size = 2\n",
    "three_d = False\n",
    "use_patches = True\n",
    "patch_size = (512, 512)\n",
    "patches_per_image = 2\n",
    "random_patches = True\n",
    "resume_from_table = False\n",
    "\n",
    "# Run batch processing\n",
    "images_list, source_desc = get_images_from_container(conn, datatype, data_id)\n",
    "if len(images_list) > 0:\n",
    "    table_id, processed_images = process_omero_batch_with_dask(\n",
    "        images_list=images_list,\n",
    "        output_folder=output_directory,\n",
    "        container_type=datatype,\n",
    "        container_id=data_id,\n",
    "        source_desc=source_desc,\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        channel=channel,\n",
    "        timepoints=timepoints,\n",
    "        timepoint_mode=timepoint_mode,\n",
    "        z_slices=z_slices,\n",
    "        z_slice_mode=z_slice_mode,\n",
    "        segment_all=segment_all,\n",
    "        train_n=train_n,\n",
    "        validate_n=validate_n,\n",
    "        three_d=three_d,\n",
    "        use_patches=use_patches,\n",
    "        patch_size=patch_size,\n",
    "        patches_per_image=patches_per_image,\n",
    "        random_patches=random_patches,\n",
    "        resume_from_table=resume_from_table,\n",
    "        read_only_mode=read_only_mode,\n",
    "        local_output_dir=local_output_dir\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
