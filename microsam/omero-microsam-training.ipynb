{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "#load dotenv for OMERO login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tifffile import imsave, imwrite, imread\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.data import MinInstanceSampler\n",
    "from torch_em.util.util import get_random_colors\n",
    "\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "conn = BlitzGateway(host=os.environ.get(\"HOST\"), username=os.environ.get(\"USER_NAME\"), passwd=os.environ.get(\"PASSWORD\"), group=os.environ.get(\"GROUP\"), secure=True)\n",
    "connection_status = conn.connect()\n",
    "if connection_status:\n",
    "    print(\"Connected to OMERO Server\")\n",
    "else:\n",
    "    print(\"Connection to OMERO Server Failed\")\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = 1112\n",
    "nucl_channel = 0\n",
    "\n",
    "#validate that data_id matches datatype\n",
    "if datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    print('Plate Name: ', plate.getName())\n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    print('Dataset Name: ', dataset.getName())\n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    print('Image Name: ', image.getName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define output folder for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "models_dir = os.path.join(home_dir, \"micro-sam_models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "folder_name = f\"micro-sam-{timestamp}\"\n",
    "output_directory = os.path.join(models_dir, folder_name)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_directory = os.path.abspath(output_directory)\n",
    "#output_directory = os.path.abspath(\"C:\\\\Users\\\\mwpaul\\\\micro-sam_models\\\\micro-sam-20250207_095503\")\n",
    "print(f\"Output directory: {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data from OMERO using the attached table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_table(conn, dataset_id, table_name=\"micro_sam_training_data\"):\n",
    "    \"\"\"\n",
    "    Find and return a specific table attached to a dataset by its name.\n",
    "    \n",
    "    Args:\n",
    "        conn: OMERO connection\n",
    "        dataset_id: ID of the dataset to search\n",
    "        table_name: Name of the table file to find\n",
    "        \n",
    "    Returns:\n",
    "        table: Table data as pandas DataFrame or list of lists\n",
    "        file_ann_id: ID of the file annotation containing the table\n",
    "    \"\"\"\n",
    "    # Get all file annotations on the dataset\n",
    "    file_ann_ids = ezomero.get_file_annotation_ids(conn, \"Dataset\", dataset_id)\n",
    "    \n",
    "    # Get original file details to check names\n",
    "    for ann_id in file_ann_ids:\n",
    "        ann = conn.getObject(\"FileAnnotation\", ann_id)\n",
    "        if ann is None:\n",
    "            continue\n",
    "            \n",
    "        orig_file = ann.getFile()\n",
    "        if orig_file.getName() == table_name:\n",
    "            try:\n",
    "                table = ezomero.get_table(conn, ann_id)\n",
    "                return table, ann_id\n",
    "            except Exception as e:\n",
    "                print(f\"Found file {table_name} but failed to load as table: {e}\")\n",
    "                continue\n",
    "                \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"micro_sam_training_data\"\n",
    "table, file_ann_id = get_specific_table(conn, data_id, table_name)\n",
    "if table is not None:\n",
    "    print(f\"Found table {table_name} in file annotation {file_ann_id}\")\n",
    "    # If pandas DataFrame:\n",
    "    print(table.head())\n",
    "else:\n",
    "    print(f\"No table named {table_name} found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download table from omero, use it to collect training data\n",
    "train_images = []\n",
    "validate_images = []\n",
    "\n",
    "folders = [\"training_input\", \"training_label\", \"val_input\", \"val_label\", \"tmp\"]\t\n",
    "for folder in folders:\n",
    "    folder = os.path.join(output_directory,folder)\n",
    "    if os.path.exists(folder) and os.path.isdir(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    #os.makedirs(folder)\n",
    "\n",
    "#prepare training data\n",
    "train_images = table[table['train'] == True]\n",
    "val_images = table[table['validate'] == True]\n",
    "os.makedirs(os.path.join(output_directory, \"tmp\"), exist_ok=True)\n",
    "training_input_dir = os.path.join(output_directory, \"training_input\")\n",
    "os.makedirs(training_input_dir, exist_ok=True)\n",
    "training_label_dir = os.path.join(output_directory, \"training_label\")\n",
    "os.makedirs(training_label_dir, exist_ok=True)\n",
    "\n",
    "for n in range(len(train_images)):\n",
    "    z_slice = train_images.iloc[n]['z_slice']\n",
    "    channel = train_images.iloc[n]['channel']\n",
    "    timepoint = train_images.iloc[n]['timepoint']\n",
    "    image = conn.getObject('Image', int(train_images.iloc[n]['image_id']))\n",
    "    pixels = image.getPrimaryPixels()\n",
    "    img = pixels.getPlane(z_slice, channel, timepoint) #(z, c, t) \n",
    "    #save image to output folder\n",
    "    # Normalize 16-bit to 8-bit using 0 as minimum\n",
    "    img_8bit = ((img) * (255.0 / img.max())).astype(np.uint8)\n",
    "\n",
    "    # Save as 8-bit tiff as required for micro-sam training\n",
    "    imwrite(os.path.join(output_directory, \"training_input\", f\"input_0000{n}.tif\"), img_8bit)\n",
    "    \n",
    "    file_path = ezomero.get_file_annotation(conn, int(train_images.iloc[n]['label_id']), os.path.join(output_directory, \"tmp\"))\n",
    "    os.rename(file_path, os.path.join(output_directory, \"training_label\", f\"label_0000{n}.tif\"))\n",
    "\n",
    "val_input_dir = os.path.join(output_directory, \"val_input\")\n",
    "os.makedirs(val_input_dir, exist_ok=True)\n",
    "val_label_dir = os.path.join(output_directory, \"val_label\")\n",
    "os.makedirs(val_label_dir, exist_ok=True) \n",
    "\n",
    "for n in range(len(val_images)):\n",
    "    image = conn.getObject('Image', int(val_images.iloc[n]['image_id']))\n",
    "    pixels = image.getPrimaryPixels()\n",
    "    img = pixels.getPlane(z_slice, channel, timepoint) #(z, c, t) \n",
    "    # Normalize 16-bit to 8-bit using 0 as minimum\n",
    "    img_8bit = ((img) * (255.0 / img.max())).astype(np.uint8)\n",
    "    #save image to output folder\n",
    "    imsave(os.path.join(output_directory, \"val_input\", f\"input_0000{n}.tif\"), img_8bit)\n",
    "    file_path = ezomero.get_file_annotation(conn, int(val_images.iloc[n]['label_id']), os.path.join(output_directory, \"tmp\"))\n",
    "    os.rename(file_path, os.path.join(output_directory, \"val_label\", f\"label_0000{n}.tif\"))\n",
    "\n",
    "print(\"Training data succesfully saved to: \", output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data loader for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # training batch size\n",
    "patch_shape = (1, 512, 512)  # the size of patches for training\n",
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "# NOTE 1: It's important to have densely annotated-labels while training the additional convolutional decoder.\n",
    "# NOTE 2: In case you do not have labeled images, we recommend using `micro-sam` annotator tools to annotate as many objects as possible per image for best performance.\n",
    "train_instance_segmentation = True\n",
    "\n",
    "# NOTE: The dataloader internally takes care of adding label transforms: i.e. used to convert the ground-truth\n",
    "# labels to the desired instances for finetuning Segment Anythhing, or, to learn the foreground and distances\n",
    "# to the object centers and object boundaries for automatic segmentation.\n",
    "\n",
    "# There are cases where our inputs are large and the labeled objects are not evenly distributed across the image.\n",
    "# For this we use samplers, which ensure that valid inputs are chosen subjected to the paired labels.\n",
    "# The sampler chosen below makes sure that the chosen inputs have atleast one foreground instance, and filters out small objects.\n",
    "sampler = MinInstanceSampler(min_size=25)  # NOTE: The choice of 'min_size' value is paired with the same value in 'min_size' filter in 'label_transform'.\n",
    "\n",
    "train_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=training_input_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=training_label_dir,\n",
    "    label_key=label_key,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    is_seg_dataset=True,\n",
    "    #rois=train_roi,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "val_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=val_input_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=val_label_dir,\n",
    "    label_key=label_key,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    is_seg_dataset=True,\n",
    "    #rois=val_roi,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    "    sampler=sampler,\n",
    ")\n",
    "check_loader(train_loader, 1, plt=True)\n",
    "check_loader(val_loader, 1, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objects_per_batch = 2  # the number of objects per batch that will be sampled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 100  # how long we train (in epochs)\n",
    "print('running on: ', device)\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = \"vit_l\"\n",
    "\n",
    "# The name of the checkpoint. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = \"sam\"\n",
    "\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(output_directory, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    #checkpoint_path='C:\\\\Users\\\\mwpaul\\\\micro-sam_models\\\\micro-sam-20250207_095503\\\\models\\\\checkpoints\\\\sam\\\\best.pt', #can be used to train further\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save as bioimage.io model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "from micro_sam.bioimageio.model_export import export_sam_model\n",
    "\n",
    "# Get a test image and label to use for exporting\n",
    "# For this example, we'll use the first image and label from validation set\n",
    "test_image_path = os.path.join(val_input_dir, os.listdir(val_input_dir)[0])\n",
    "test_label_path = os.path.join(val_label_dir, os.listdir(val_label_dir)[0])\n",
    "\n",
    "# Load the test image and label\n",
    "test_image = np.array(imread(test_image_path))\n",
    "test_label = np.array(imread(test_label_path))\n",
    "\n",
    "# Define the path for saving the bioimage.io model\n",
    "bioimageio_model_path = os.path.join(output_directory, \"bioimage_io_model\")\n",
    "os.makedirs(bioimageio_model_path, exist_ok=True)\n",
    "\n",
    "# Export the SAM model to bioimage.io format\n",
    "export_sam_model(\n",
    "    image=test_image,\n",
    "    label_image=test_label,\n",
    "    model_type=model_type,  # Using the same model_type as in training\n",
    "    name=f\"micro_sam_{timestamp}\",\n",
    "    output_path=bioimageio_model_path,\n",
    "    checkpoint_path=os.path.join(\n",
    "        output_directory, \"models\", \"checkpoints\", checkpoint_name, \"best.pt\"\n",
    "    ),\n",
    "    # Optional: Add additional kwargs as needed\n",
    "    authors=[{\"name\": \"Your Name\", \"affiliation\": \"Your Institution\"}],\n",
    "    description=\"Micro-SAM model trained on microscopy images for segmentation\",\n",
    "    license=\"MIT\",\n",
    "    documentation=\"Model trained with micro-sam for segmenting microscopy images\",\n",
    ")\n",
    "\n",
    "print(f\"BioImage.IO model exported to: {bioimageio_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
