{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning SAM with OMERO data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- clean up tmp files when not neccesary anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OMERO Python BlitzGateway\n",
    "import omero\n",
    "from omero.gateway import BlitzGateway\n",
    "import ezomero\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import Python System Packages\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tifffile import imwrite\n",
    "\n",
    "#micro-sam related imports\n",
    "from micro_sam.sam_annotator import annotator_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection with OMERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "conn = BlitzGateway(host='localhost', username='root', passwd='omero', secure=True)\n",
    "print(conn.connect())\n",
    "conn.c.enableKeepAlive(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name:  day7_testing\n"
     ]
    }
   ],
   "source": [
    "datatype = \"dataset\" # \"plate\", \"dataset\", \"image\"\n",
    "data_id = 253\n",
    "nucl_channel = 0\n",
    "\n",
    "#validate that data_id matches datatype\n",
    "if datatype == \"plate\":\n",
    "    plate = conn.getObject(\"Plate\", data_id)\n",
    "    print('Plate Name: ', plate.getName())\n",
    "elif datatype == \"dataset\":\n",
    "    dataset = conn.getObject(\"Dataset\", data_id)\n",
    "    print('Dataset Name: ', dataset.getName())\n",
    "elif datatype == \"image\":\n",
    "    image = conn.getObject(\"Image\", data_id)\n",
    "    print('Image Name: ', image.getName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from OMERO and open in napari with micro-sam annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Directory:  c:\\users\\mwpaul\\appdata\\local\\temp\\tmp6oy21oa0\n"
     ]
    }
   ],
   "source": [
    "new_output_directory = os.path.normcase(tempfile.mkdtemp())\n",
    "print('Output Directory: ', new_output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputation took 183.96702575683594 seconds (= 03:04 minutes)\n",
      "The first image to annotate is image number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 44/2000 [2:36:50<116:12:33, 213.88s/it]\n",
      "Epoch 0:   5%|▌         | 100/2000 [4:51:22<92:16:08, 174.83s/it]\n",
      "C:\\Users\\mwpaul\\AppData\\Local\\Temp\\ipykernel_26932\\2881332177.py:74: UserWarning: Refusing to run a QApplication with no topLevelWidgets. To run the app anyway, use `run(force=True)`\n",
      "  napari.run()\n"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "from napari.settings import get_settings\n",
    "import zipfile\n",
    "from micro_sam.sam_annotator import image_series_annotator\n",
    "from micro_sam.util import precompute_image_embeddings\n",
    "\n",
    "def zip_directory(folder_path, zip_file):\n",
    "    for folder_name, subfolders, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            # Create complete filepath of file in directory\n",
    "            file_path = os.path.join(folder_name, filename)\n",
    "            # Add file to zip\n",
    "            zip_file.write(file_path)\n",
    "def interleave_arrays(train_images, validate_images):\n",
    "    \"\"\"\n",
    "    Interleave two arrays of images in the pattern: train[0], validate[0], train[1], validate[1], ...\n",
    "    If arrays are of unequal length, remaining elements are appended at the end.\n",
    "    \"\"\"\n",
    "    # Create empty list to store interleaved images\n",
    "    interleaved = []\n",
    "    sequence = []\n",
    "    # Get the length of the longer array\n",
    "    max_len = max(len(train_images), len(validate_images))\n",
    "    \n",
    "    # Interleave the arrays\n",
    "    for i in range(max_len):\n",
    "        # Add train image if available\n",
    "        if i < len(train_images):\n",
    "            interleaved.append(train_images[i])\n",
    "            sequence.append(0)\n",
    "        # Add validate image if available\n",
    "        if i < len(validate_images):\n",
    "            interleaved.append(validate_images[i])\n",
    "            sequence.append(1)\n",
    "    \n",
    "    return np.array(interleaved), np.array(sequence)\n",
    "\n",
    "\n",
    "##imput parameters\n",
    "model_type = 'vit_l'\n",
    "train_n = 3\n",
    "validate_n = 3\n",
    "channel = 3 #which channel to segment starting from 0\n",
    "timepoint = 0\n",
    "z_slice = 5 #for now pick one slice but TODO add option to pick multiple slices by giving a list of z slices\n",
    "\n",
    "#set napari settings\n",
    "settings = get_settings()\n",
    "settings.application.ipy_interactive = False\n",
    "#for now just get first image in dataset\n",
    "if datatype == \"dataset\":\n",
    "    images_dataset = list(dataset.listChildren())\n",
    "    images = []\n",
    "    for image in images_dataset:\n",
    "        pixels = image.getPrimaryPixels()\n",
    "        img = pixels.getPlane(z_slice, channel, timepoint) #(z, c, t) \n",
    "        #precompute_image_embeddings(img, model_type='vit_l', save_path=new_output_directory)\n",
    "        images.append(img)\n",
    "    #start napari viewer\n",
    "    viewer = napari.Viewer()\n",
    "    output_folder = new_output_directory\n",
    "    os.makedirs(os.path.join(output_folder,\"output\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_folder,\"embed\"), exist_ok=True)\n",
    "    image_series_annotator(images, model_type=model_type, viewer=viewer,embedding_path=os.path.join(output_folder,\"embed\"),output_folder=os.path.join(output_folder,\"output\"))\n",
    "    napari.run()\n",
    "    # Wait until the napari viewer is closed by the user\n",
    "    #TODO clean up napari output in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ID:  1343\n"
     ]
    }
   ],
   "source": [
    "if datatype == \"dataset\":    \n",
    "    embed_ids = []\n",
    "    for n,image in enumerate(combined_images):\n",
    "        embed_file = os.path.join(output_folder,\"embed\",f\"embedding_{n:05d}.zarr\")#fixed leading zeros\n",
    "        #zip zarr directory\n",
    "        zip_path = os.path.join(output_folder,\"embed\",f\"embedding_{n:05d}.zip\")\n",
    "        zip_file = zipfile.ZipFile(zip_path, 'w')\n",
    "        zip_directory(embed_file, zip_file)\n",
    "        zip_file.close()\n",
    "        #upload zip file as attachment to image\n",
    "        file_annotation_id = ezomero.post_file_annotation(\n",
    "            conn,\n",
    "            str(zip_path),\n",
    "            ns='microsam.embeddings',\n",
    "            object_type=\"Image\",\n",
    "            object_id=image.getId(),\n",
    "            description='image embedding') #TODO add specification of type of embedding etc\n",
    "        embed_ids.append(file_annotation_id)\n",
    "    #upload annotations as attachment to images\n",
    "    label_ids = []\n",
    "    for n,image in enumerate(combined_images):\n",
    "        label_file = os.path.join(output_folder,\"output\",f\"seg_{n:05d}.tif\")#fixed leading zeros\n",
    "        #upload label file as attachment to image\n",
    "        file_annotation_id = ezomero.post_file_annotation(\n",
    "            conn,\n",
    "            str(label_file),\n",
    "            ns='microsam.labelimage',\n",
    "            object_type=\"Image\",\n",
    "            object_id=image.getId(),\n",
    "            description='label image') #TODO add specification of type of label embedding etc\n",
    "        label_ids.append(file_annotation_id)\n",
    "\n",
    "#upload table with training data\n",
    "df = pd.DataFrame(columns=[\"image_id\", \"image_name\", \"train\", \"validate\", \"channel\", \"timepoint\", \"sam_model\"])\n",
    "for n, image in enumerate(combined_images):\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"image_id\": image.getId(),\n",
    "        \"image_name\": image.getName(),\n",
    "        \"train\": combine_images_sequence[n] == 0,\n",
    "        \"validate\": combine_images_sequence[n] == 1,\n",
    "        \"channel\": channel,\n",
    "        \"timepoint\": timepoint,\n",
    "        \"sam_model\": model_type,\n",
    "        \"embed_id\": embed_ids[n],\n",
    "        \"label_id\": label_ids[n]\n",
    "    }])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "tabelid = ezomero.post_table(conn, object_type=\"Dataset\", object_id=data_id, table = df,title=\"micro_sam_training_data\")\n",
    "print(\"Table ID: \", tabelid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
