{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imsave\n",
    "import torch\n",
    "\n",
    "from tifffile import imwrite\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.data import MinInstanceSampler\n",
    "from torch_em.util.util import get_random_colors\n",
    "\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define in/output folder for training\n",
    "Select a folder on you local computer. The folder needs to contain the following folders:\n",
    "  \n",
    "**root_folder/**  \n",
    "→ training_input/  - images used for training  \n",
    "→ training_label/  - matching label images, should have the same file names  \n",
    "→ val_input/       - images used for validation  \n",
    "→ val_label/       - matching validation images  \n",
    "→ model/           - empty folder which will contain the model we will train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.abspath(\"C:\\\\Users\\\\mwpaul\\\\micro-sam_models\\\\micro-sam-20250207_095503\")\n",
    "print(f\"Output directory: {output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the checkpoint/model name. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = \"sam\"\n",
    "n_objects_per_batch = 2  # the number of objects per batch that will be sampled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 100  # how long we train (in epochs)\n",
    "print('running on: ', device)\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = \"vit_l\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data loader for the training\n",
    "This will return a few examples to make sure data is properly organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_dir = os.path.join(output_directory, \"val_input\")\n",
    "os.makedirs(val_input_dir, exist_ok=True)\n",
    "val_label_dir = os.path.join(output_directory, \"val_label\")\n",
    "os.makedirs(val_label_dir, exist_ok=True) \n",
    "\n",
    "batch_size = 2  # training batch size\n",
    "patch_shape = (1, 512, 512)  # the size of patches for training\n",
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "# NOTE 1: It's important to have densely annotated-labels while training the additional convolutional decoder.\n",
    "# NOTE 2: In case you do not have labeled images, we recommend using `micro-sam` annotator tools to annotate as many objects as possible per image for best performance.\n",
    "train_instance_segmentation = True\n",
    "\n",
    "# NOTE: The dataloader internally takes care of adding label transforms: i.e. used to convert the ground-truth\n",
    "# labels to the desired instances for finetuning Segment Anythhing, or, to learn the foreground and distances\n",
    "# to the object centers and object boundaries for automatic segmentation.\n",
    "\n",
    "# There are cases where our inputs are large and the labeled objects are not evenly distributed across the image.\n",
    "# For this we use samplers, which ensure that valid inputs are chosen subjected to the paired labels.\n",
    "# The sampler chosen below makes sure that the chosen inputs have atleast one foreground instance, and filters out small objects.\n",
    "sampler = MinInstanceSampler(min_size=25)  # NOTE: The choice of 'min_size' value is paired with the same value in 'min_size' filter in 'label_transform'.\n",
    "\n",
    "train_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=training_input_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=training_label_dir,\n",
    "    label_key=label_key,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    is_seg_dataset=True,\n",
    "    #rois=train_roi,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "val_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=val_input_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=val_label_dir,\n",
    "    label_key=label_key,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    is_seg_dataset=True,\n",
    "    #rois=val_roi,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    "    sampler=sampler,\n",
    ")\n",
    "check_loader(train_loader, 1, plt=True)\n",
    "check_loader(val_loader, 1, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(output_directory, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    #checkpoint_path='C:\\\\...\\\\models\\\\checkpoints\\\\sam\\\\best.pt', #can be used to train further\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
